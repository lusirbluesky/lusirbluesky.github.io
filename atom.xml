<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>phoenix&#39;s blog</title>
  
  <subtitle>In the future you will appreciate your desperate effort now</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://www.bluedreams.top/"/>
  <updated>2018-03-19T06:26:03.004Z</updated>
  <id>http://www.bluedreams.top/</id>
  
  <author>
    <name>phoenix</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>JVM专题</title>
    <link href="http://www.bluedreams.top/2018/03/19/jvm-%E5%8F%82%E6%95%B0/"/>
    <id>http://www.bluedreams.top/2018/03/19/jvm-参数/</id>
    <published>2018-03-19T06:23:03.196Z</published>
    <updated>2018-03-19T06:26:03.004Z</updated>
    
    <content type="html"><![CDATA[<p>#JVM常用优化参数</p><p>JVM常用参数说明，这里是以Java7为例（Java8有部分参数已失效）：</p><p>##1、参数说明：</p><p>-Xms1024M：初始化堆内存大小（注意，不加M的话单位是KB）<br>-Xmx1024M：最大堆内存大小 (1g=1024m)<br>-XX:PermSize=256M：初始化类加载内存池大小<br>-XX:MaxPermSize=256M：最大类加载内存池大小<br>-XX:MaxnewSize=256M：表示新生代可被分配的内存的最大上限；当然这个值应该小于 -Xmx的值；<br>-Xss512k：设置每个线程的堆栈大小。JDK5.0以后每个线程堆栈大小为1M，以前每个线程堆栈大小为256K。</p><p>##2、java 调用配置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java -Xms1g -Xmx2g -XX:PermSize=256m -XX:MaxNewSize=256m -XX:MaxPermSize=256m -Xss512k -cp ./java-training.jar com.ganymede.oop.Client</span><br></pre></td></tr></table></figure><p>##3、Tomcat的JVM配置</p><p>在tomcat_home/bin目录下找到catalina.bat</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set JAVA_OPTS= -Xms1g -Xmx2g -XX:PermSize=256m -XX:MaxNewSize=256m -XX:MaxPermSize=256m -Xss512k</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;#JVM常用优化参数&lt;/p&gt;
&lt;p&gt;JVM常用参数说明，这里是以Java7为例（Java8有部分参数已失效）：&lt;/p&gt;
&lt;p&gt;##1、参数说明：&lt;/p&gt;
&lt;p&gt;-Xms1024M：初始化堆内存大小（注意，不加M的话单位是KB）&lt;br&gt;-Xmx1024M：最大堆内存大小 (1
      
    
    </summary>
    
      <category term="jvm" scheme="http://www.bluedreams.top/categories/jvm/"/>
    
    
      <category term="jvm" scheme="http://www.bluedreams.top/tags/jvm/"/>
    
  </entry>
  
  <entry>
    <title>JVM专题</title>
    <link href="http://www.bluedreams.top/2018/03/19/jvm-GC/"/>
    <id>http://www.bluedreams.top/2018/03/19/jvm-GC/</id>
    <published>2018-03-19T06:15:13.445Z</published>
    <updated>2018-03-19T06:20:45.370Z</updated>
    
    <content type="html"><![CDATA[<p>#JVM常用算法</p><p>关于GC(Garbage Collection)的算法，常用的有以下几种：</p><p>##1、引用计数法</p><p>1) 老牌垃圾回收算法，通过引用计算来回收垃圾。引用计数器的实现很简单，对于一个对象A，只要有任何一个对象引用了A，则A的引用计数器就加1，当引用失效时，引用计数器就减1。只要对象A的应用计数器的值为0，则对象A就不可能再被使用。</p><blockquote><p>使用引用计数法的语言：</p><p>Micsoft COM</p><p>ActionScript3</p><p>Python</p></blockquote><p>2) 引用计数法的问题:</p><p>引用和去引用伴随加法和减法，影响性能</p><p>很难处理循环引用</p><p>3) 由于引用计数法存在的问题，因此JVM并未使用该GC算法。</p><p>##2、标记-清除算法</p><p><strong>标记-清除算法是现代垃圾回收算法的思想基础。</strong></p><p>标记-清除算法将垃圾回收分为两个阶段：标记阶段和清除阶段。一种可行的实现是，在标记阶段，首先通过根节点，标记所有从根节点开始的可达对象。因此，未被标记的对象就是未被引用的垃圾对象。然后，在清除阶段，清除所有未被标记的对象。</p><p>##3、标记-压缩算法</p><p><strong>标记-压缩算法适合用于存活对象较多的场合，如老年代。</strong>它在标记-清除算法的基础上做了一些优化。和标记-清除算法一样，标记-压缩算法也首先需要从跟节点开始，对所有可达对象做一次标记。但之后，它并不简单的清理未标记的对象，而是将所有的存活对象压缩到内存的一端。之后，清理边界外所有的空间。</p><p>与标记-除算法相比，标记-压缩算法处理了内存中的碎片，GC后让存活的内存放在连续的空间内，更好地利用了内存</p><p>##4、复制算法</p><p>复制算法与标记-清除算法相比是一种相对高效的回收方法</p><p>不适用于存活对象较多的场合 如老年代。</p><p>复制算法将原有的内存空间分为两块，每次只使用其中一块，在垃圾回收时，将正在使用的内存中的存活对象复制到未使用的内存块中，之后，清除正在使用的内存块中的所有对象，交换两个内存的角色，完成垃圾回收</p><p><strong>复制算法常用于新生代</strong></p><p>依据对象的存活周期进行分类，短命对象归为新生代，长命对象归为老年代。</p><p>根据不同代的特点，选取合适的收集算法</p><p>少量对象存活，适合复制算法，如新生代</p><p>大量对象存活，适合标记清理或者标记压缩，如老年代。</p><p>##5、总结JVM使用的GC算法</p><p>1）<strong>老年代通常采用标记-压缩算法</strong>，如果老年代使用CMS回收器，则先使用标记-清除算法，然后再定时对内存进行压缩整理。</p><p>2）<strong>新生代通常使用复制算法</strong></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;#JVM常用算法&lt;/p&gt;
&lt;p&gt;关于GC(Garbage Collection)的算法，常用的有以下几种：&lt;/p&gt;
&lt;p&gt;##1、引用计数法&lt;/p&gt;
&lt;p&gt;1) 老牌垃圾回收算法，通过引用计算来回收垃圾。引用计数器的实现很简单，对于一个对象A，只要有任何一个对象引用了A，则
      
    
    </summary>
    
      <category term="jvm" scheme="http://www.bluedreams.top/categories/jvm/"/>
    
    
      <category term="jvm" scheme="http://www.bluedreams.top/tags/jvm/"/>
    
  </entry>
  
  <entry>
    <title>Spark</title>
    <link href="http://www.bluedreams.top/2018/03/12/sparkonyarn/"/>
    <id>http://www.bluedreams.top/2018/03/12/sparkonyarn/</id>
    <published>2018-03-11T23:00:52.818Z</published>
    <updated>2018-03-11T23:03:19.136Z</updated>
    
    <content type="html"><![CDATA[<p>#Spark</p><h2 id="Spark的显著特点"><a href="#Spark的显著特点" class="headerlink" title="Spark的显著特点"></a>Spark的显著特点</h2><blockquote><p>分布式</p><p>基于内存（部分基于磁盘）</p><p>迭代式计算</p></blockquote><p>##SparkSQL能取代Hive吗？</p><blockquote><p>不能。Hive包含两部分：计算引擎和存储仓库。SparkSQL只能替换Hive的计算引擎，不能取代Hive。Hive是一个很好的数据仓库。</p><p>spark处理的数据来源：HDFS、HBase、Hive、DB、Local FS、S3等</p><p>spark处理的数据输出：HDFS、HBase、Hive、DB（s3等）、spark客户端（正在处理任务的机器Driver）</p></blockquote><p>##RDD创建的方式</p><blockquote><p>1.外部数据来源（如HDFS）</p><p>2.根据Scala集合</p><p>3.由其他RDD操作（Transformation操作会产生新的RDD，Action操作不会产生新的RDD）</p><p>数据会被RDD划分为一系列的Partitions，分配到每个Partition的数据属于一个Task的处理范畴</p><p>RDD为什么有很多不同的创建方式？</p><p>因为Spark会基于不同的介质进行运算</p></blockquote><p>##Spark弹性分布式数据集RDD</p><blockquote><p>弹性1：    自动进行内存和磁盘的数据存储读取的切换</p><p>弹性2：基于Lineage(血统)的高效容错（计算错误或丢失不会从头开始，第n个节点出错，会从第n-1个节点开始计算，极大提高了错误的恢复的速度）</p><p>弹性3：Task如果失败会自动进行特定次数的重试（默认4次）（只计算失败的阶段）</p><p>弹性4：Stage如果失败会自动进行特定次数的重试（默认3次）（只计算失败的分片）</p><p>弹性5：设置持久化和检查点（persistent和checkpoint，计算链条比较长或者计算比较笨重，考虑用checkpoint将数据都放在磁盘上，persist是在内存或磁盘进行数据复用。这是效率和容错的延伸点）</p><p>弹性6：数据调度弹性：DAGTask和资源管理无关</p><p>弹性7：数据分片的高度弹性（在计算过程中可能会产生很多数据碎片，这样一个partition会特别小，这么小的partition也要消耗一个线程去处理会降低处理效率，这时要考虑将很多小的partition合并成一个大的partition进行处理以提升效率；另外，如果内存不太多，但是每个partition特别大，数据block比较大，要考虑把他变成更小的分片这样让spark有更多的处理批次但是不会有OOM。数据分片提高并行度或降低并行度的弹性表现，而且数据处理的时候完全有数据本地性，并行度设置要人工处理。100w的数据分片变成1w个，不要调用repartition，要使用coalesce）</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="comment">/**</span></span><br><span class="line"><span class="comment">&gt;    * Return a new RDD that has exactly numPartitions partitions.</span></span><br><span class="line"><span class="comment">&gt;    *</span></span><br><span class="line"><span class="comment">&gt;    * Can increase or decrease the level of parallelism in this RDD. Internally, this uses</span></span><br><span class="line"><span class="comment">&gt;    * a shuffle to redistribute data.</span></span><br><span class="line"><span class="comment">&gt;    *</span></span><br><span class="line"><span class="comment">&gt;    * If you are decreasing the number of partitions in this RDD, consider using `coalesce`,</span></span><br><span class="line"><span class="comment">&gt;    * which can avoid performing a shuffle.</span></span><br><span class="line"><span class="comment">&gt;    */</span></span><br><span class="line">&gt;   <span class="function"><span class="keyword">def</span> <span class="title">repartition</span></span>(numPartitions: <span class="type">Int</span>)(<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>] = <span class="literal">null</span>): <span class="type">RDD</span>[<span class="type">T</span>] = withScope &#123;</span><br><span class="line">&gt;     coalesce(numPartitions, shuffle = <span class="literal">true</span>)</span><br><span class="line">&gt;   &#125;</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><p>&gt;</p><blockquote><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="comment">/**</span></span><br><span class="line"><span class="comment">&gt;    * Return a new RDD that is reduced into `numPartitions` partitions.</span></span><br><span class="line"><span class="comment">&gt;    *</span></span><br><span class="line"><span class="comment">&gt;    * This results in a narrow dependency, e.g. if you go from 1000 partitions</span></span><br><span class="line"><span class="comment">&gt;    * to 100 partitions, there will not be a shuffle, instead each of the 100</span></span><br><span class="line"><span class="comment">&gt;    * new partitions will claim 10 of the current partitions.</span></span><br><span class="line"><span class="comment">&gt;    *</span></span><br><span class="line"><span class="comment">&gt;    * However, if you're doing a drastic coalesce, e.g. to numPartitions = 1,</span></span><br><span class="line"><span class="comment">&gt;    * this may result in your computation taking place on fewer nodes than</span></span><br><span class="line"><span class="comment">&gt;    * you like (e.g. one node in the case of numPartitions = 1). To avoid this,</span></span><br><span class="line"><span class="comment">&gt;    * you can pass shuffle = true. This will add a shuffle step, but means the</span></span><br><span class="line"><span class="comment">&gt;    * current upstream partitions will be executed in parallel (per whatever</span></span><br><span class="line"><span class="comment">&gt;    * the current partitioning is).</span></span><br><span class="line"><span class="comment">&gt;    *</span></span><br><span class="line"><span class="comment">&gt;    * <span class="doctag">Note:</span> With shuffle = true, you can actually coalesce to a larger number</span></span><br><span class="line"><span class="comment">&gt;    * of partitions. This is useful if you have a small number of partitions,</span></span><br><span class="line"><span class="comment">&gt;    * say 100, potentially with a few partitions being abnormally large. Calling</span></span><br><span class="line"><span class="comment">&gt;    * coalesce(1000, shuffle = true) will result in 1000 partitions with the</span></span><br><span class="line"><span class="comment">&gt;    * data distributed using a hash partitioner.</span></span><br><span class="line"><span class="comment">&gt;    */</span></span><br><span class="line">&gt;   <span class="function"><span class="keyword">def</span> <span class="title">coalesce</span></span>(numPartitions: <span class="type">Int</span>, shuffle: <span class="type">Boolean</span> = <span class="literal">false</span>,</span><br><span class="line">&gt;                partitionCoalescer: <span class="type">Option</span>[<span class="type">PartitionCoalescer</span>] = <span class="type">Option</span>.empty)</span><br><span class="line">&gt;               (<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>] = <span class="literal">null</span>)</span><br><span class="line">&gt;       : <span class="type">RDD</span>[<span class="type">T</span>] = withScope &#123;</span><br><span class="line">&gt;       <span class="comment">///////////</span></span><br><span class="line">&gt; &#125;</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><p>&gt;</p><blockquote><p>（task重试失败，整个计算阶段stage就会失败，计算阶段有很多并行的数据分片，这些分片的计算逻辑相同但是处理的数据不同，会再次提交stage只把失败的任务提交，已经成功的任务不会再次提交（这些成功的任务已经有结果输出，如果没有结果还是也会被提交））</p></blockquote><p>spark的中间数据可以在内存也可在磁盘，不一定全部在内存</p><p>RDD在抽象上表示了元素的集合，内部包含数据，数据本身分区，在不同的节点上可以被并行操作，所以叫分布式数据集。最常用的sparkRDD的数据来源hadoop上，spark+hadoop黄金组合。</p><p>在开发或测试时，要启动<strong>start-history-server.sh</strong>，他会记录程序运行的信息，这样在session会话结束后还可以看曾经运行的程序的情况。可以在浏览器通过spark的地址+18080端口查看</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark-shell --master spark://node01:7077</span><br></pre></td></tr></table></figure><p>RDD通过spark context创建</p><p>spark context是集群的唯一的接口，你做一切工作都要基于spark context</p><p>spark的一切操作皆RDD</p><p>RDD可以从HDFS或者Hive表中获取数据</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">手动指定集群地址</span></span><br><span class="line">sc.textFile('hdfs://node01:9000/input')；</span><br><span class="line"><span class="meta">#</span><span class="bash">根据上下文自动获取集群地址</span></span><br><span class="line">val data = sc.textFile('/input')；</span><br><span class="line">data.toDebugString</span><br><span class="line"><span class="meta">#</span><span class="bash">Transformation算子（没有结果，lazy）-textFile</span></span><br><span class="line"><span class="meta">#</span><span class="bash">Action算子(给出结果，hungry)-count</span></span><br><span class="line">data.count</span><br><span class="line"><span class="meta">#</span><span class="bash">spark界面的Tasks中的Locality_Level显示</span></span><br><span class="line"><span class="meta">#</span><span class="bash">NODE_LOCAL表示在磁盘</span></span><br><span class="line"><span class="meta">#</span><span class="bash">PROCESS_LOCAL表示在内存</span></span><br><span class="line"><span class="meta">#</span><span class="bash">ANY表示shuffle过的数据</span></span><br><span class="line"><span class="meta">#</span><span class="bash">计算不复杂的，数据大部分在磁盘上，对于迭代操作默认放在内存</span></span><br><span class="line">val flatted=data.flatMap(_.split(" "))</span><br><span class="line"><span class="meta">#</span><span class="bash">通过执行下面的语句，发现RDD是有依赖关系的</span></span><br><span class="line">flatted.toDebugString</span><br><span class="line">val mapped = flatted.map(word=&gt;(word,1))</span><br><span class="line">mapped.toDebufString</span><br><span class="line">val reduced = mapped.reduceByKey(_+_)</span><br><span class="line">reduced.toDebugString</span><br><span class="line">reduced.saveAsTextFile("/output/data")</span><br></pre></td></tr></table></figure><p>##SparkContext的作用？</p><blockquote><p>初始化spark应用程序运行所需要的核心组件，包括DAGScheduler、TaskScheduler、SchedulerBackend；同时还会负责spark程序往Master注册程序等。SparkContext是spark应用程序中最为重要的一个对象。</p><p>DAGScheduler：高层调度器，划分任务的不同阶段</p><p>TaskScheduler:接口，根据不同的资源调度模式</p><p>资源调度模式：local模式，standalone模式，yarn模式，mesos模式，ec2模式（云计算）</p></blockquote><p>RDD本身是泛型，是一系列的数据分片，每个分片是具体类型String或其他</p><figure class="highlight plain"><figcaption><span>data:RDD[String] </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">##spark的每个步骤是不是都要做缓存？</span><br><span class="line"></span><br><span class="line">&gt; 不是每个步骤都会缓存，否则内存损耗太严重，这就和hadoop的mapreduce差不多了。</span><br><span class="line">&gt;</span><br><span class="line">&gt; 需要做缓存的情况：</span><br><span class="line">&gt;</span><br><span class="line">&gt; 1.计算步骤特别耗时</span><br><span class="line">&gt;</span><br><span class="line">&gt; 2.计算链条很长</span><br><span class="line">&gt;</span><br><span class="line">&gt; 3.shuffle之后</span><br><span class="line">&gt;</span><br><span class="line">&gt; （shuffle是从其他阶段拉数据，这时缓存，就不要重新shuffle）</span><br><span class="line">&gt;</span><br><span class="line">&gt; 4.checkpoint之前</span><br><span class="line"></span><br><span class="line">spark on yarn时，container是一个线程池</span><br><span class="line"></span><br><span class="line">一个操作可能产生一个RDD也可能产生多个RDD，如：textFile()产生了两个RDD（HadoopRDD和MapPartitionsRDD）</span><br><span class="line"></span><br><span class="line">HadoopRDD有key-行索引，value-行的内容</span><br><span class="line"></span><br><span class="line">MapPartitionsRDD没有key，只有value</span><br><span class="line"></span><br><span class="line">reduceByKey产生了两个RDD（MapPartitionsRDD和ShuffledRDD）</span><br><span class="line"></span><br><span class="line">##Spark的partition数量怎么确定？什么时候需要repartition？</span><br><span class="line"></span><br><span class="line">&gt; 根据实际运行的状况设定，根据Core的个数，CPU的个数，集群的资源等。</span><br><span class="line">&gt;</span><br><span class="line">&gt; 需要repartition:</span><br><span class="line">&gt;</span><br><span class="line">&gt; - 数据碎片太多（要把多个碎片合并）</span><br><span class="line">&gt; - 要增加分片数量（OOM，内存不够）</span><br><span class="line"></span><br><span class="line">##ShuffledRDD之后还有一个MapPartitionsRDD，从保存数据到HDFS的角度讲MapPartitionsRDD？</span><br><span class="line"></span><br><span class="line">spark在计算的时候，把key暂时丢掉了，最后存储到HDFS上，要按照saveAsHadoopFile的要求的格式，将key补上，key不是偏移量，为什么需要它，因为hadoop需要</span><br><span class="line"></span><br><span class="line">```scala</span><br><span class="line">def saveAsTextFile(path:String)&#123;</span><br><span class="line">this.map(x=&gt;(NullWritable.get(),new Text(x.toString))</span><br><span class="line">    .saveAsHadoopFile[TextOutputFormat[NullWritable, Text]](path)</span><br><span class="line">&#125;</span><br><span class="line">///////////////////</span><br><span class="line">val r = this.mapPartitions &#123; iter =&gt;</span><br><span class="line">      val text = new Text()</span><br><span class="line">      iter.map &#123; x =&gt;</span><br><span class="line">        text.set(x.toString)</span><br><span class="line">        (NullWritable.get(), text)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    RDD.rddToPairRDDFunctions(r)(nullWritableClassTag, textClassTag, null)</span><br><span class="line">      .saveAsHadoopFile[TextOutputFormat[NullWritable, Text]](path)</span><br></pre></td></tr></table></figure><p>##wordcount的第一个Stage和第二个Stage产生了哪些RDD？</p><blockquote><p>stage1:</p><p>HadoopRDD</p><p>MapPartitionsRDD</p><p>MapPartitionsRDD</p><p>MapPartitionsRDD</p><p>stage2:</p><p>ShuffledRDD</p><p>MapPartitionsRDD</p></blockquote><p>reduceByKey包含local reduce和global reduce两个部分</p><p>spark中master和worker管理的资源主要指内存和CPU</p><p>spark的active状态的master指：现在正在管理集群并接受外界程序的程序提交请求和资源分配请求的master</p><p>standby模式的master随时准备在active模式master挂掉后代替它管理系群</p><p>active模式的master挂掉后zookeeper自动切换</p><p>spark的HA可以是两台spark也可以是多台</p><p>##zookeeper中的元数据信息包括哪些内容？</p><blockquote><p>所有worker、driver、application的元数据</p></blockquote><p>##spark中的资源分配粒度？</p><blockquote><p>粗粒度：一次性分配任务所需的全部资源</p><p>细粒度：分配一部分，用完重新申请</p><p>一般的是粗粒度</p></blockquote><p>Spark中资源的分配是在程序注册时完成的，不是在具体的工作过程中申请分配的。</p><p>注册完成后资源被谁管理（standalone模式）：CoarseGrainedSchedulerBackend的子类SparkDeploySchedulerBackend</p><p>driver到master注册并初始化</p><p>Spark的角色？</p><blockquote><p>master</p><p>worker</p><p>driver：驱动程序的运行</p><p>executor：具体运行任务，计算数据</p></blockquote><p>一个应用程序默认有一个DAGScheduler</p><p>一个worker节点默认为当前应用程序开启一个executor</p><p>worker管理executor</p><p>worker管理当前node的资源并接受master的指令来分配具体的计算资源executor（在新的进程中分配）</p><p>ExecutorRunner相当于一个proxy，管理新分配的进程并监控该executor分配的进程的运行状况</p><p>worker会向master发送心跳信息（只有workerid），但不会向master汇报当前节点内存和CPU的信息。</p><p>master怎么知道worker节点的资源信息？</p><p>应用程序在master上注册时，注册成功就会分配资源，分配的时候就记住了worker的资源状况，所有资源都是master分配的。</p><p>job是包含了一系列task的并行计算，在spark中一般由action触发，触发的时候前面会有一系列的RDD，action不会产生RDD，它会导致runjob。action前面的RDD是transaction级别的，lazy级别，没有计算。</p><p>spark的快不是因为基于内存，当前RDD回溯到前一个RDD（如果是窄依赖在内存中进行迭代，这是spark快的一个重要的原因，但spark快绝对不是因为基于内存），速度快有很多原因，最基本的是调度机制和容错机制。</p><p>窄依赖除了一对一的依赖还有range级别的依赖固定个数的父RDD的partition，依赖的固定个数不会随着计算规模的大小而改变，DAG就会变成宽依赖（依赖构成了DAG），如果是宽依赖DAGSchedule就会帮我们划分Stage，Stage内部是基于内存迭代的，也可以基于磁盘迭代。</p><p>Stage内部：计算逻辑完全一样，只是计算的数据不同罢了。</p><p>一个Application里面可以有多个不同的job，因为可以有不同的action，而一般一个action操作就会对应一个job。checkpoint也会导致job，在进行排序算法的时候进行range范围的划分也会触发job</p><p>##为什么不能再IDE集成开发环境中直接发布Spark程序到Spark集群？</p><blockquote><p>1.内存和cores的限制，默认spark程序的driver会在提交spark程序的机器上，所以如果在IDE中提交程序的话，那IDE机器必须非常强大</p><p>2.driver要指挥workers的运行并频繁的发生通信，如果开发环境IDE与spark集群不在同样的网络环境，就会出现任务丢失、运行缓慢等多种不必要的问题</p><p>3.这是不安全的</p></blockquote><p>##spark的底层是akka，为啥spark1.6.x内部实现通过rpc？</p><p> 以后可能不用akka了</p><p>现在是做了一层封装</p><p>spark2.0底层改用了netty</p><p>driver能像master一样做HA吗？</p><blockquote><p>client模式不行</p><p>cluster模式下，–spark-submit提交参数可指定–supervise后，如果driver挂掉后集群会自动帮你启动driver</p></blockquote><p>##spark怎么知道分配多少资源？</p><blockquote><p>1.spark-env.sh和spark-default.sh</p><p>2.spark-submit提供的参数</p><p>3.程序中的sparkConf配置的参数</p></blockquote><p>RDD：基于工作集的应用抽象</p><p>hadoop的mapreduce是基于数据集的处理</p><p>不论是基于工作集还是数据集，他们的共同特征：数据感知，容错，负载均衡</p><blockquote><p>基于数据集的处理（代表是hadoop得到mapreduce）：</p><p>1.从物理存储设备加载数据</p><p>2.操作数据</p><p>3.写入物理设备</p><p>（spark的调度框架也是基于hadoop的mapreduce的一种实现，只是更细致，更高效）</p><p>不适合的场景：</p><ul><li><p>不适合大量迭代（迭代就是每一步对数据进行相似的函数，机器学习或处理数据算法比较复杂）</p></li><li><p>不适合交互式查询（每次查询都要从磁盘读取数据，处理，写入磁盘）</p></li><li><p>致命点：基于数据流的方式不能复用曾经的结果或者中间计算结果</p><blockquote><p>比如：1000个人对数据仓库进行并发查询，</p><p>查询完全一样，每次都要重新查询</p><p>查询中前10个步骤一样，后面几个步骤不同</p></blockquote></li></ul><p>spark的RDD是基于工作集</p><p>RDD（Resilient Distribute Dataset）：上面总结五点弹性</p><p>RDD就是一个List或者Array</p><p>RDD是分布式函数式编程的抽象（基于RDD的编程一般都会产生高阶函数，简单的可能不会产生）</p><p>每步操作都是基于RDD操作，RDD又是只读分区的集合，但是每步操作都会改变数据（map,reduceBykey…），这时产生中间结果怎么办？不能立即计算，lazy级别，只标记不计算。最后action算子操作（回溯）触发所有链条函数执行，函数展开，计算结果。</p><p><strong>spark比hadoop的位置感知好很多，hadoop进行partitioner之后就不管reducer在哪里了，但是spark进行partitioner，进行下一步stage操作时会确定位置</strong></p></blockquote><p>spark streaming用到窗口机制，为什么要checkpoint？</p><p>checkpoint要将数据或结果保存到磁盘上</p><p>因为要经常用到以前的东西</p><p>如果一个stage有1000个步骤，spark不会产生999次中间结果，默认只会产生一次中间结果，hadoop的mapreduce会产生999次中间结果。</p><blockquote><p>每步操作都是基于RDD操作，RDD又是只读分区的集合，但是每步操作都会改变数据（map,reduceBykey…），这时产生中间结果怎么办？不能立即计算，lazy级别，只标记不计算。</p></blockquote><p>数据量越大，计算步骤越多，spark的优势越明显。</p><p>spark1.3版本里程碑，dataframe的革命</p><p>spark1.2.x版本前，由于shuffle问题，spark不适合大规模计算，有规模限制，但是之后任意规模，可以自定义shuffle插件</p><p>常规容错的方式：数据检查点（每次拷贝跨网络传输，消耗存储）和记录数据更新操作（每次数据变化都记录，复杂，难处理重新计算）</p><p>##Spark的容错：通过RDD记录数据更新的方式为何高效？</p><blockquote><p>1.RDD不可变+Lazy级别构成了链条</p><p>2.RDD的写操作是粗粒度（提高效率，简化复杂度），RDD的读操作可粗粒度可细粒度（RDD的大多数场景都是粗粒度）</p></blockquote><p>从第900步恢复的前提是要在第900步持久化？</p><p>第900步是上一个stage的结束或者第900步进行了checkpoint/persist持久化。（stage结束会写入磁盘）</p><p>没有中间结果不需要进行维护</p><p>RDD的数据分片上的一系列的计算逻辑都是一样的</p><p>Tachyon解决南北（不同）数据中心数据不同步问题</p><p>Spark的所有子框架都是基于RDD的</p><p>spark要做一体化多元化的通用数据处理框架，兼容一切操作系统，文件格式，数据库</p><p>spark不能处理的问题：实时事务性处理</p><p>spark要统一数据计算领域，除了实时事务性处理</p><p>##RDD的缺陷</p><blockquote><p>不支持细粒度的更新操作</p><p>不支持增量迭代计算</p></blockquote><p>##RDD为什么有很多不同的创建方式？</p><p>因为Spark会基于不同的介质进行运算</p><p>##Spark和Hadoop有关系吗？</p><p>没有。Spark运行在Hadoop之上，hadoop作为spark的数据存储来源的时候有关系。</p><p>##RDD的创建方式</p><blockquote><p>三种最基本的方式创建：</p><p>1.使用程序中的集合创建RDD</p><p>2.使用本地文件系统创建RDD</p><p>3.使用HDFS创建RDD</p><p>其他创建方式：</p><p>4.基于DB创建RDD</p><p>5.基于NoSQL，如HBase</p><p>6.基于S3创建</p><p>7.基于数据流创建RDD（如spark streaming）</p><p>通过集合创建RDD的实际意义？</p><blockquote><p>测试</p></blockquote><p>使用本地文件系统创建RDD的作用？</p><blockquote><p>测试大量数据文件</p></blockquote><p>使用HDFS创建RDD？</p><blockquote><p>生产环境最常用的RDD创建方式</p></blockquote></blockquote><p>##Spark最佳实践</p><blockquote><p>并行度设置</p><p>单个core可以承载2-4个partition（32core可以承载64-128partition）</p><blockquote><p>任务的数据量大小及复杂程度涉及到内存和CPU</p></blockquote></blockquote><p>##RDD的操作有几种？</p><blockquote><p>三种。</p><p>transformation</p><p>针对已有的RDD创建一个新的RDD</p><p>action</p><p>得到最终结果</p><p>controller</p><ul><li>控制算子</li><li>cache，persist，checkpoint（cache是一种特殊的persist）</li></ul></blockquote><p>##谈谈Shuffle</p><blockquote><p><strong>什么是shuffle？</strong></p><p>洗牌，需要shuffle的关键原因是：某种需要共同特征的数据需要最终汇聚到 一个计算节点上进行计算。</p><p><strong>什么时候会产生shuffle？</strong></p><p>运行task的时候才会产生shuffle（shuffle已经融合在spark的算子中了）</p><p><strong>shuffle可能面临的问题？</strong></p><p>1.数据量非常大（网络传输）</p><p>2.数据如何分类，即如何partition</p><p>分为hash partition，sort partition和钨丝计划</p><p>3.负载均衡（数据倾斜）</p><p>4.网络传输效率，需要在压缩与解压缩之间权衡，序列化和反序列化</p><p><strong>说明：</strong>具体的task进行计算的时候，尽一切可能使数据具备process locality的特性；退而求其次，增加数据分片，减少每个task处理的数据量</p><ul><li>persist或cache的价值不如出错后重新在内存算一遍（cache的风险OOM或空间被别人占用）</li><li>内存中重新计算比将结果持久化到磁盘然后从磁盘读结果可能更快</li><li>读磁盘IO是高风险的，读内存风险降低很多</li><li>一般不持久化中间结果，数据丢失重新计算依赖的RDD，在一个Stage内部鼓励这样做</li><li>如果产生shuffle，会进行网络通信，这时肯定要持久化，一般持久化到磁盘，也可以持久化的内存等</li></ul><p><strong>hash shuffle</strong></p><p>1.key不能是Array(如果是array，不能友好的计算hashcode)</p><p>2.hash shuffle不需要排序（理论上速度快，性能好），此时从理论上讲节省了hadoop mapreduce进行shuffle排序的时间浪费，因为实际生产环境有大量不需要排序的shuffle</p><p>思考：不需要排序的hash shuffle一定比需要排序的sorted shuffle速度快？</p><blockquote><p>不一定。</p><p>数据规模小时，hash shuffle会比sorted shuffle快（很多）</p><p>数据量大时，sorted shuffle一般都会比hash shuffle快（很多）</p><p>因为hash方式会有key，句柄，有一些小文件等，磁盘和内存会变成瓶颈。sorted会极大的节省内存和对磁盘的访问，更有利于大规模的数据计算。</p></blockquote><p>3.除了最后一个stage里面的任务是result task，其他前面的都是shuffle map task。每个shufflemaptask会根据key的hash值计算出当前的key需要的partition，然后把决定后的结果写入单独的文件。。。。。。。。。</p><p>注意：shuffle操作绝大多数情况下要经过网络，如果mapper和reducer在同一台机器上，此时只需读取本地磁盘。</p><p>hash shuffle的两大死穴：</p><ul><li>shuffle前会在磁盘上产生海量的小文件，此时会产生大量耗时低效的IO操作；</li><li>内存不共用！由于内存中需要保存海量文件操作句柄和临时缓存信息，如果数据处理规模比较庞大的话，内存不可承受，出现OOM等问题。</li></ul><p>为了改善上述问题（同时打开过多文件导致Write Handler内存使用过大以及产生过度文件导致大量随机读写带来的低效磁盘IO），spark推出了consalidate机制，合并小文件。此时shuffle时产生的文件的数量为<strong>cores*R</strong>，对于shuffle map task的数量明显多于同时可用的并行cores的数量时，shuffle产生的文件数量会大幅度减少，会极大降低OOM可能</p><p><strong>为什么使用sort-based shuffle？</strong></p><p>1.shuffle一般包含两阶段的任务，产生shuffle数据的阶段（map阶段）和使用shuffle数据的阶段 （reduce阶段）</p><p><strong>sort-based shuffle</strong></p><p>归并排序</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;#Spark&lt;/p&gt;
&lt;h2 id=&quot;Spark的显著特点&quot;&gt;&lt;a href=&quot;#Spark的显著特点&quot; class=&quot;headerlink&quot; title=&quot;Spark的显著特点&quot;&gt;&lt;/a&gt;Spark的显著特点&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;分布式&lt;/p&gt;
&lt;p&gt;
      
    
    </summary>
    
      <category term="bigdata" scheme="http://www.bluedreams.top/categories/bigdata/"/>
    
    
      <category term="bigdata" scheme="http://www.bluedreams.top/tags/bigdata/"/>
    
  </entry>
  
  <entry>
    <title>Spark性能优化</title>
    <link href="http://www.bluedreams.top/2018/03/12/spark/"/>
    <id>http://www.bluedreams.top/2018/03/12/spark/</id>
    <published>2018-03-11T22:55:10.446Z</published>
    <updated>2018-03-11T22:54:03.985Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Spark性能优化"><a href="#Spark性能优化" class="headerlink" title="Spark性能优化"></a>Spark性能优化</h1><p>##Spark性能优化的10大问题及其解决方案</p><blockquote><p>问题1：reduce task数目不合适</p><p>解决方案：</p><p>需要根据实际情况调整默认配置，调整方式是修改参数spark.default.parallelism。通常的，reduce数目设置为core数目的2-3倍。数量太大，造成很多小任务，增加启动任务的开销；数目太小，任务运行缓慢。所以要合理修改reduce的task数目即spark.default.parallelism</p><p>问题2：shuffle磁盘IO时间长</p><p>解决方案：</p><p>设置spark.local.dir为多个磁盘，并设置磁盘的IO速度快的磁盘，通过增加IO来优化shuffle性能；</p><p>问题3：map|reduce数量大，造成shuffle小文件数目多</p><p>解决方案：</p><p>通过设置spark.shuffle.consolidateFiles为true，来合并shuffle中间文件，此时文件数为reduce tasks数目；</p><p>问题4：序列化时间长、结果大</p><p>解决方案：</p><p>spark默认使用JDK 自带的ObjectOutputStream，这种方式产生的结果大、CPU处理时间长，可以通过设置spark.serializer为org.apache.spark.serializer.KeyoSerializer。</p><p>另外如果结果已经很大，那就最好使用广播变量方式了，结果你懂得。</p><p>问题5：单条记录消耗大</p><p>解决方案：</p><p>使用mapPartition替换map，mapPartition是对每个Partition进行计算，而map是对partition中的每条记录进行计算；</p><p>问题6 : collect输出大量结果时速度慢</p><p>解决方案：</p><p>collect源码中是把所有的结果以一个Array的方式放在内存中，可以直接输出到分布式的文件系统，然后查看文件系统中的内容；</p><p>问题7: 任务执行速度倾斜</p><p>解决方案：</p><p>如果数据倾斜，一般是partition key取得不好，可以考虑其他的并行处理方式，并在中间加上aggregation操作；如果是Worker倾斜，例如在某些Worker上的executor执行缓慢，可以通过设置spark.speculation=true 把那些持续慢的节点去掉；</p><p>问题8: 通过多步骤的RDD操作后有很多空任务或者小任务产生</p><p>解决方案：</p><p>使用coalesce或者repartition去减少RDD中partition数量；</p><p>问题9：Spark Streaming吞吐量不高</p><p>可以设置spark.streaming.concurrentJobs</p><p>问题10：Spark Streaming 运行速度突然下降了，经常会有任务延迟和阻塞</p><p>解决方案：</p><p>这是因为我们设置job启动interval时间间隔太短了，导致每次job在指定时间无法正常执行完成，换句话说就是创建的windows窗口时间间隔太密集了；</p><p>问题11：receiver个数设置</p></blockquote><p>##Spark SQL之Join</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Spark性能优化&quot;&gt;&lt;a href=&quot;#Spark性能优化&quot; class=&quot;headerlink&quot; title=&quot;Spark性能优化&quot;&gt;&lt;/a&gt;Spark性能优化&lt;/h1&gt;&lt;p&gt;##Spark性能优化的10大问题及其解决方案&lt;/p&gt;
&lt;blockquote&gt;
&lt;
      
    
    </summary>
    
      <category term="bigdata" scheme="http://www.bluedreams.top/categories/bigdata/"/>
    
    
      <category term="bigdata" scheme="http://www.bluedreams.top/tags/bigdata/"/>
    
  </entry>
  
  <entry>
    <title>hive</title>
    <link href="http://www.bluedreams.top/2018/03/12/hive/"/>
    <id>http://www.bluedreams.top/2018/03/12/hive/</id>
    <published>2018-03-11T22:55:10.415Z</published>
    <updated>2018-03-11T22:48:34.559Z</updated>
    
    <content type="html"><![CDATA[<p>#hive</p><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><ul><li>自行下载安装包</li><li>上传<strong>hive-1.2.1.tar.gz</strong>包到<strong>/export/softwares/</strong>目录</li><li>解压<strong>hive-1.2.1.tar.gz</strong>包到<strong>/export/servers/</strong>目录</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf /export/softwares/hive-1.2.1.tar.gz /export/servers/</span><br></pre></td></tr></table></figure><ul><li>安装mysql数据库</li></ul><blockquote><p>推荐yum 在线安装</p></blockquote><ul><li>修改配置文件</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 配置HIVE_HOME环境变量</span></span><br><span class="line">vi conf/hive-env.sh </span><br><span class="line"><span class="meta">#</span><span class="bash"> 配置其中的<span class="variable">$hadoop_home</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Set HADOOP_HOME to point to a specific hadoop install directory</span></span><br><span class="line">export HADOOP_HOME=/export/servers/hadoop</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 配置元数据库信息</span></span><br><span class="line">vi  conf/hive-site.xml</span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!--添加如下内容：--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://localhost:3306/hive?createDatabaseIfNotExist=true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span>JDBC connect string for a JDBC metastore<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span>Driver class name for a JDBC metastore<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span>username to use against metastore database<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>sql<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span>password to use against metastore database<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li><p>安装hive和mysql完成后，将mysql的连接jar包拷贝到$HIVE_HOME/lib目录下，如果出现没有权限的问题，在mysql授权(在安装mysql的机器上执行)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mysql -uroot -p</span><br><span class="line">#(执行下面的语句 *.*:所有库下的所有表%：任何IP地址或主机都可以连接)</span><br><span class="line">GRANT ALL PRIVILEGES ON *.* TO &apos;root&apos;@&apos;%&apos; IDENTIFIED BY &apos;root&apos; WITH GRANT OPTION;</span><br><span class="line">FLUSH PRIVILEGES;</span><br></pre></td></tr></table></figure></li><li><p>Jline包版本不一致的问题</p></li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 需要拷贝hive的lib目录中jline.2.12.jar的jar包替换掉hadoop中的</span></span><br><span class="line">/export/servers/hadoop/share/hadoop/yarn/lib/jline-0.9.94.jar</span><br></pre></td></tr></table></figure><h2 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h2><ul><li>hive交互shell</li></ul><blockquote><p>bin/hive</p></blockquote><ul><li>hive JDBC服务(参考java jdbc连接mysql)</li><li>hive启动为一个服务器，来对外提供服务</li></ul><blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash"> <span class="comment">#启动hiveserver2</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> <span class="comment"># 前台启动</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> bin/hiveserver2</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> <span class="comment"># 后台启动</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> nohup bin/hiveserver2 1&gt;/var/<span class="built_in">log</span>/hiveserver.log 2&gt;/var/<span class="built_in">log</span>/hiveserver.err &amp;</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"></span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash"> <span class="comment"># beeline连接</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> <span class="comment"># 启动成功后，可以在别的节点上用beeline去连接</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> bin/beeline -u jdbc:hive2://node01:10000 -n root</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> <span class="comment">#或者</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> bin/beeline ! connect jdbc:hive2://node01:10000</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> <span class="comment"># 用户名密码为hiveserver2所在节点的linux用户名密码</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> <span class="comment"># 如果不做特别权限配置，只需要输入用户名，密码直接回车即可</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"></span></span><br></pre></td></tr></table></figure></blockquote><ul><li>hive命令</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 在linux环境下执行hive sql语句，执行完成回到linux环境</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> hive -e <span class="string">'sql'</span></span></span><br><span class="line">bin/hive -e 'select * from t_test'</span><br></pre></td></tr></table></figure><p><strong>提示：hive的使用依赖hadoop环境，所以要启动hdfs和yarn</strong></p><p>##hive基本操作</p><p>###DDL操作</p><p>sql</p><p>创建表加载数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#数据文件user.txt</span><br><span class="line">#1tom21</span><br><span class="line">#2david23</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--创建数据库lusir</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">database</span> lusir;</span><br><span class="line"><span class="keyword">use</span> lusir;</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--创建数据表t_user</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> t_user(<span class="keyword">id</span> <span class="built_in">int</span>,<span class="keyword">name</span> <span class="keyword">string</span>,age <span class="built_in">int</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">上传到对应的数据表下建立映射关系</span></span><br><span class="line">hadoop fs -put /root/data/user.txt /user/hive/warehouse/lusir.db/t_user</span><br><span class="line"><span class="meta">#</span><span class="bash">或者通过加载数据</span></span><br><span class="line">load data local inpath '/root/data/user.txt'</span><br><span class="line">[overwrite] into table t_user;</span><br><span class="line"><span class="meta">#</span><span class="bash">查询</span></span><br><span class="line">select * from t_user;</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--jerrybeijing,tianjin</span></span><br><span class="line"><span class="comment">--tomshanghai,nanjing</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> complex_array(<span class="keyword">name</span> <span class="keyword">string</span>,work_locations <span class="built_in">array</span>&lt;<span class="keyword">string</span>&gt;)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="string">'\t'</span></span><br><span class="line">collection items <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">','</span>;</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--1,zhangsan,唱歌:非常喜欢-跳舞:喜欢-游泳:一般般</span></span><br><span class="line"><span class="comment">--2,lisi,打游戏:非常喜欢-篮球:不喜欢</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> t_map(<span class="keyword">id</span> <span class="built_in">int</span>,<span class="keyword">name</span> <span class="keyword">string</span>,hobby <span class="keyword">map</span>&lt;<span class="keyword">string</span>,<span class="keyword">string</span>&gt;)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> </span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">','</span></span><br><span class="line">collection items <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'-'</span></span><br><span class="line"><span class="keyword">map</span> <span class="keyword">keys</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">':'</span> ;</span><br></pre></td></tr></table></figure><p>创建分区表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--创建分区表</span></span><br><span class="line"><span class="comment">--单分区</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> t_user(<span class="keyword">id</span> <span class="built_in">int</span>,<span class="keyword">name</span> <span class="keyword">string</span>)</span><br><span class="line">partitioned <span class="keyword">by</span> (country <span class="keyword">string</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">','</span>;、</span><br><span class="line"><span class="comment">--注释：分区字段不能和表中实际字段重名，否则报错</span></span><br><span class="line"><span class="comment">--FAILED: SemanticException [Error 10035]: Column repeated in partitioning columns</span></span><br><span class="line"><span class="comment">--加载本地数据</span></span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/root/data/partition.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> t_user <span class="keyword">partition</span>(country=<span class="string">'china'</span>);</span><br><span class="line"><span class="comment">--注释：加载本地数据时，数据文件必须在hiveserver2所在节点的本地存在</span></span><br><span class="line"><span class="comment">--注释：hdfs上对应的数据库文件夹下的新文件夹为country=china</span></span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--查看表信息</span></span><br><span class="line">desc t_users;</span><br><span class="line"><span class="keyword">describe</span> t_users;</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--创建分区表</span></span><br><span class="line"><span class="comment">--双分区</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> day_hour_table(<span class="keyword">id</span> <span class="built_in">int</span>,<span class="keyword">name</span> <span class="keyword">string</span>)</span><br><span class="line">partitioned <span class="keyword">by</span> (dt <span class="keyword">string</span>,<span class="keyword">hour</span> <span class="keyword">string</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">','</span>;</span><br><span class="line"><span class="comment">--加载(hdfs数据)</span></span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> inpath <span class="string">'/hivedata/double.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> day_hour_table <span class="keyword">partition</span>(dt=<span class="string">'2018-02-26'</span>,<span class="keyword">hour</span>=<span class="string">'14'</span>);</span><br><span class="line"><span class="comment">--查看分区表的分区信息</span></span><br><span class="line"><span class="keyword">show</span> <span class="keyword">partitions</span> day_hour_table;</span><br><span class="line"><span class="comment">--查看非分区表的分区信息（报错）</span></span><br><span class="line"><span class="keyword">show</span> <span class="keyword">partitions</span> ttt;</span><br></pre></td></tr></table></figure><p>创建分桶表</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">指定开启分桶</span></span><br><span class="line">set hive.enforce.bucketing = true;</span><br><span class="line">set mapreduce.job.reduces=4;</span><br><span class="line"><span class="meta">#</span><span class="bash">默认值<span class="built_in">set</span> mapreduce.job.reduces=-1</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="built_in">set</span> mapreduce.job.reduces与mapreduce的reducetask的个数（分区数）相同</span></span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--创建分桶表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> stu_buck(Sno <span class="built_in">int</span>,Sname <span class="keyword">string</span>,Sex <span class="keyword">string</span>,Sage <span class="built_in">int</span>,Sdept <span class="keyword">string</span>)</span><br><span class="line">clustered <span class="keyword">by</span>(Sno)</span><br><span class="line"><span class="comment">--sorted by(Sno DESC)</span></span><br><span class="line"><span class="keyword">into</span> <span class="number">4</span> buckets</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">','</span>;</span><br><span class="line"><span class="comment">--加载数据不会分桶（数据展现就像普通表）</span></span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/root/data/buck.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> stu_buck;</span><br><span class="line"><span class="comment">--分桶表导入数据</span></span><br><span class="line"><span class="comment">--借助导入另一张表数据</span></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> stu_buck</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> student cluster <span class="keyword">by</span>(Sno);</span><br><span class="line"><span class="comment">--直接导入自身数据</span></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> stu_buck</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> stu_buck cluster <span class="keyword">by</span>(Sno);</span><br></pre></td></tr></table></figure><p>内部表<strong>vs</strong>外部表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--建内部表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> student(Sno <span class="built_in">int</span>,Sname <span class="keyword">string</span>,Sex <span class="keyword">string</span>,Sage <span class="built_in">int</span>,Sdept <span class="keyword">string</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">','</span>;</span><br><span class="line"><span class="comment">--建外部表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> student_ext(Sno <span class="built_in">int</span>,Sname <span class="keyword">string</span>,Sex <span class="keyword">string</span>,Sage <span class="built_in">int</span>,Sdept <span class="keyword">string</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">','</span> location <span class="string">'/stu'</span>;</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--内、外部表加载数据：</span></span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/root/hivedata/students.txt'</span> overwrite <span class="keyword">into</span> <span class="keyword">table</span> student;</span><br><span class="line"></span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> inpath <span class="string">'/stu'</span> <span class="keyword">into</span> <span class="keyword">table</span> student_ext;</span><br></pre></td></tr></table></figure><p>like创建表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--LIKE 允许用户复制现有的表结构，但是不复制数据</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> like_tbl <span class="keyword">like</span> t_user;</span><br></pre></td></tr></table></figure><p>###DML操作</p><p>insert查询语句</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--多重插入：</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> source_table (<span class="keyword">id</span> <span class="built_in">int</span>, <span class="keyword">name</span> <span class="keyword">string</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">','</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> test_insert1 (<span class="keyword">id</span> <span class="built_in">int</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">','</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> test_insert2 (<span class="keyword">name</span> <span class="keyword">string</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">','</span>;</span><br><span class="line"></span><br><span class="line">from source_table</span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> test_insert1</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">id</span></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> test_insert2</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">name</span>;</span><br></pre></td></tr></table></figure><p>动态分区插入</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">是否开启动态分区功能，默认<span class="literal">false</span>关闭。</span></span><br><span class="line">set hive.exec.dynamic.partition=true;</span><br><span class="line"><span class="meta">#</span><span class="bash">动态分区的模式，默认strict，表示必须指定至少一个分区为静态分区<span class="comment">#nonstrict模式表示允许所有的分区字段都可以使用动态分区。</span></span></span><br><span class="line">set hive.exec.dynamic.partition.mode=nonstrict;</span><br></pre></td></tr></table></figure><p>需求：<br>将dynamic_partition_table中的数据按照时间(day)，插入到目标表d_p_t的相应分区中。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">#dynamic_partition_table.txt</span><br><span class="line">2015-05-10,ip1</span><br><span class="line">2015-05-10,ip2</span><br><span class="line">2015-06-14,ip3</span><br><span class="line">2015-06-14,ip4</span><br><span class="line">2015-06-15,ip1</span><br><span class="line">2015-06-15,ip2</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--原始表：</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> dynamic_partition_table(<span class="keyword">day</span> <span class="keyword">string</span>,ip <span class="keyword">string</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">","</span>;</span><br><span class="line"><span class="comment">--加载数据</span></span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/root/hivedata/dynamic_partition_table.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> dynamic_partition_table;</span><br><span class="line"></span><br><span class="line"><span class="comment">--目标表：</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> d_p_t(ip <span class="keyword">string</span>) partitioned <span class="keyword">by</span> (<span class="keyword">month</span> <span class="keyword">string</span>,<span class="keyword">day</span> <span class="keyword">string</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">--动态插入：</span></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> d_p_t <span class="keyword">partition</span> (<span class="keyword">month</span>,<span class="keyword">day</span>) </span><br><span class="line"><span class="keyword">select</span> ip,<span class="keyword">substr</span>(<span class="keyword">day</span>,<span class="number">1</span>,<span class="number">7</span>) <span class="keyword">as</span> <span class="keyword">month</span>,<span class="keyword">day</span> </span><br><span class="line"><span class="keyword">from</span> dynamic_partition_table;</span><br><span class="line"><span class="comment">--动态分区是通过位置来对应分区值的。原始表select 出来的值和输出partition的值的关系仅仅是通过位置来确定的，和名字并没有关系。</span></span><br></pre></td></tr></table></figure><p>查询结果导出到文件系统</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--将查询结果保存到指定的文件目录（可以是本地，也可以是hdfs）</span></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">local</span> <span class="keyword">directory</span> <span class="string">'/home/hadoop/test'</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> t_p;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">directory</span> <span class="string">'/aaa/test'</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> t_p;</span><br></pre></td></tr></table></figure><blockquote><p>小结</p><p><strong>hive</strong>数据存储在<strong>hdfs</strong>的<strong>/user/hive/warehouse</strong>路径下</p><p>请注意查看hive-site.xml中的配置</p><p>数据库，数据表，分区，分桶在hdfs中以文件夹的形式存在，数据以文件的形式存在</p><p>元数据默认存储在内置的derby数据库中，在哪个路径下启动hive都会产生一份元数据，带来了很多不便。为了解决这种问题，用mysql中永久保存元数据信息，以免每次启动hive都要重新创建。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash"> <span class="comment"># 默认数据库路径（default数据库）</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> /user/hive/warehouse/</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> <span class="comment"># 自建数据库路径</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> /user/hive/warehouse/lusir.db</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> <span class="comment"># 自建数据库路径下的表</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> /user/hive/warehouse/lusir.db/<span class="built_in">test</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> <span class="comment"># 自建数据库路径下的表下的数据文件（0,1或多个文件）</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> /user/hive/warehouse/lusir.db/<span class="built_in">test</span>/data.txt</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"></span></span><br></pre></td></tr></table></figure></blockquote><p>&gt;</p><blockquote><p><strong>hive中的表实际上是hdfs中指定路径下的数据文件中的结构化数据与mysql中的元数据信息映射之后的结果</strong></p><p>完美产生映射的必要条件</p><ul><li>表的字段个数和类型跟结构化数据中的字段个数和类型一致</li><li>在建表的时候指定本次映射的结构化数据中的分隔符是什么</li></ul><blockquote><p>默认分隔符<strong>“\001”</strong>，在数据文件直接手动敲出无效，要在vim编辑环境下用ctrl+v然后ctrl+a得到<strong>^A</strong>即为默认分隔符。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash"> &gt; <span class="comment">#指定分隔符</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> &gt; <span class="comment">#指定分隔符为\t</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> &gt; row format delimited fields terminated by <span class="string">'\t'</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> &gt;</span></span><br></pre></td></tr></table></figure></blockquote></blockquote><p>&gt;</p><blockquote><p>后续所有针对结构化数据的操作分析，转化成了对这个hive中表的分析</p><p>sql由hive帮我们转化成了MapReduce程序</p><p>分区</p><ul><li>hive中的分区字段不能够是表当中已经存在的字段</li><li>分区的字段是个虚拟的字段不存放任何数据</li><li>它的值来自于你装载分区表数据的时候指定的</li><li>它的效果就是在表的文件夹下面又创建了子文件夹</li><li>目的：进一步划分数据，避免查询对全表扫描（只扫描指定的分区即可）即提高查询效率</li></ul><p>分桶</p><ul><li>分桶操作之前需要手动开启分桶的功能同时指定分为几个桶</li><li>hive中的分桶（分簇）字段必须是表当中已经存在的字段</li><li>针对分桶表的数据导入：<figure class="highlight plain"><figcaption><span>data ```的方式不行，原因在于load数据本质相当于由hive帮你去```hadoop fs -put xxx数据文件```这没有执行**MR**程序，不会分桶</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt; - 分桶表的数据插入要使用 insert+select方式：插入的数据来自于查询语句，本质是分桶一定要执行**MR**程序，对应**MR**中Partitioner</span><br><span class="line">&gt; - 默认分桶规则按照你指定的分桶字段**模除以**（%）设置的分桶个数</span><br><span class="line">&gt;</span><br><span class="line">&gt; ```shell</span><br><span class="line">&gt;  set mapreduce.job.reduces=？</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></li></ul></blockquote><p>&gt;</p><blockquote><ul><li>分桶虽然也是针对数据进行更加细致化的划分，但是更多的是用来（Map端连接操作）<strong>提高多表之间的join查询效率</strong></li><li><strong>需要join的字段在各自表中进行分桶</strong></li><li>避免了全表笛卡尔积，可以直接匹配，所以更加高效</li></ul><p>在数据量足够大的情况下，分桶比分区更高的查询效率</p><blockquote><p>索引分区分桶小结</p><p><strong>索引和分区最大的区别就是索引不分割数据库，分区分割数据库。</strong></p><p>索引其实就是拿额外的存储空间换查询时间，但分区已经将整个大数据库按照分区列拆分成多个小数据库了。</p><p><strong>分区和分桶最大的区别就是分桶随机分割数据库，分区是非随机分割数据库。</strong></p><p>因为分桶是按照列的哈希函数进行分割的，相对比较平均；而分区是按照列的值来进行分割的，容易造成数据倾斜。</p><p>其次两者的另一个区别就是分桶是对应不同的文件（细粒度），分区是对应不同的文件夹（粗粒度）。</p><p>注意：普通表（外部表、内部表）、分区表这三个都是对应HDFS上的目录，桶表对应是目录里的文件</p><p><strong>推荐大家学习的时候HIVE、HDFS、MYSQL三者结合一起看，更容易理解其中的原理。</strong></p><p><a href="https://www.cnblogs.com/LiCheng-/p/7420888.html" target="_blank" rel="noopener">HIVE-索引、分区和分桶的区别</a></p></blockquote><p><strong>hive中的表名和字段名对大小写不敏感</strong></p><p><strong>hive中类sql语句要以分号标识一个语句的结束</strong></p><p>加载本地数据时，数据文件必须在hiveserver2所在节点的本地存在</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; load data local inpath '' into table tablename</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><p>&gt;</p><blockquote><p>加载集群数据，节点必须开启hiveserver2且能够访问hdfs集群且hdfs指定路径存在对应的数据文件</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; load data inpath '' into table tablename</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><p>&gt;</p><blockquote><p><strong>分区表是在目录级别进行划分</strong></p><p><strong>分桶表是在文件级别进行划分</strong></p><p>分区用于创建表，加载表，插入表</p><p>分桶用于创建表，查询表</p><p>加载数据注意：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash"> <span class="comment">#使用local会将数据文件复制到hdfs的hive对应数据表</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> load data <span class="built_in">local</span> inpath <span class="string">'/root/data/1.txt'</span> into table tt;</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> <span class="comment">#不用local而直接加载hdfs数据会将数据文件移动到hive对应数据表</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> <span class="comment">#当映射路径既有文件又有文件夹时，文件会全部映射到hive对应表中，文件夹如果为空会报错</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> load data inpath <span class="string">'/hive/'</span> overwrite into table tt;</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"></span></span><br></pre></td></tr></table></figure></blockquote><p>&gt;</p><blockquote><p>hive命令行中可执行hive sql命令，hdfs命令和linux shell命令</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt; --hive中执行hdfs命令</span><br><span class="line">&gt; dfs -ls /</span><br><span class="line">&gt; --hive中执行linux shell命令</span><br><span class="line">&gt; !ls /root/</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><p>&gt;</p><blockquote><p>hive加载数据时不做类型检查，查询时检查</p><p>hive中select查询插入数据注意字段位置</p><p>外部分区表需要先添加分区才能看到数据</p><p><strong>导出数据</strong></p><p>导出路径不存在自动创建<br>数据导出到本地支持指定分隔符<br>数据导出到hdfs不支持指定分隔符</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&gt; --数据导出到本地</span><br><span class="line">&gt; insert into|overwrite local directory '/root/data/'</span><br><span class="line">&gt; row format delimited fields terminated by '\t'</span><br><span class="line">&gt; select name,addr</span><br><span class="line">&gt; from test;</span><br><span class="line">&gt; --数据导出到hdfs</span><br><span class="line">&gt; insert into|overwrite directory '/hdfs/data/'</span><br><span class="line">&gt; select name,addr</span><br><span class="line">&gt; from test;</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><p>&gt;</p><blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash"> <span class="comment">#hadoop命令导出数据</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> hadoop fs -get</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> hadoop fs -text </span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"></span></span><br></pre></td></tr></table></figure></blockquote><p>&gt;</p><blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash"> <span class="comment">#shell命令加管道</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> <span class="comment">#第三方工具Sqoop导出数据到本地</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"></span></span><br></pre></td></tr></table></figure></blockquote><p>&gt;</p><blockquote><p>一个表一天产生的分区数最好不要太多</p><p>动态分区</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&gt; #是否开启动态分区功能，默认false关闭。</span><br><span class="line">&gt; set hive.exec.dynamic.partition=true;</span><br><span class="line">&gt; #动态分区的模式，默认strict，表示必须指定至少一个分区为静态分区#nonstrict模式表示允许所有的分区字段都可以使用动态分区。</span><br><span class="line">&gt; set hive.exec.dynamic.partition.mode=nonstrict;</span><br><span class="line">&gt;</span><br><span class="line">&gt; create table t_part(name string)</span><br><span class="line">&gt; partitioned by (val String)</span><br><span class="line">&gt; row format delimited</span><br><span class="line">&gt; fields terminated by '\t'</span><br><span class="line">&gt; lines terminated '\n'</span><br><span class="line">&gt; stored as teextfile;</span><br><span class="line">&gt;</span><br><span class="line">&gt; insert overwrite table t_part partition(val)</span><br><span class="line">&gt; select name,addr as val</span><br><span class="line">&gt; from test;</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><p>&gt;</p><blockquote><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt; alter table test rename to newtable;</span><br><span class="line">&gt; alter table test change column c1 c2 int comment '年龄';</span><br><span class="line">&gt; alter table test add columns (c1 string comment '',c2 long comment '');</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><p>&gt;</p><blockquote><p>hive不支持union，支持union all</p><p><strong>count(*)&amp;count(1)</strong></p><blockquote><p>count(*)当该记录的所有值都不为NULL才计数，都为NULL不计数</p><p>count(1)不管记录中是否有NULL，都计数</p></blockquote><p>但是，<a href="https://my.oschina.net/rathan/blog/608617" target="_blank" rel="noopener">HQL中count(*),count(1),count(column)的区别</a>一文中写的却不同，该文结论如下：</p><blockquote><p>count(*)：所有行进行统计，包括NULL行</p><p>count(1)：所有行进行统计，包括NULL行</p><p>count(column)：对column中非Null进行统计</p><p>count(distinct column)：对column中非Null进行去重统计</p></blockquote><p>where语句是在map阶段执行</p><p>一般不在大数据中使用order by，将数据处理完成后，导出到关系型数据库，在关系型数据库中使用order by</p><p>select后面的非聚合列必须出现在group by中，否则报错</p><p>having是在reduce端执行的，在reduce端执行完聚合操作之后进行的</p><p>优化group by参数：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt; --开启两个job</span><br><span class="line">&gt; set hive.groupby.skewindata=true;</span><br><span class="line">&gt; --数据倾斜优化</span><br><span class="line">&gt; set mapred.reduce.tasks=5;</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><p>&gt;</p><blockquote><p>执行join时存在数据倾斜，设置优化参数：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; set hive.optimize.skewjoin=true;</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><p>&gt;</p><blockquote><p>一般的join操作是在reduce端进行join的，mapjoin是一种优化操作，在map端进行join操作，优化join的查询。</p><blockquote><p>什么是mapjoin？</p><ul><li>在map端把小表加载到内存，然后读取大表再和内存中的小表进行连接操作</li><li>使用了分布式缓存技术</li></ul><p>mapjoin的优缺点？</p><blockquote><p>优点：</p><ul><li>不消耗集群的reduce资源（reduce相对紧缺时）</li><li>减少reduce操作，加快程序执行</li><li>降低网络负载</li></ul><p>缺点：</p><ul><li>占用部分内存，所以加载到内存中的表不能过大，因为每个计算节点都会加载一次（最好几M到几十M）</li><li>生成较多的小文件（与Map的任务数有关）</li></ul></blockquote><p>使用mapjoin的参数设置？</p><ul><li>配置参数，hive根据sql自动选择使用commonjoin或mapjoin</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt; &gt; --Whether Hive enables the optimization about converting common join into mapjoin based on the input file size. (Note that hive-default.xml.template incorrectly gives the default as false in Hive 0.11.0 through 0.13.1.)</span><br><span class="line">&gt; &gt; set hive.auto.convert.join=true;</span><br><span class="line">&gt; &gt; --Default Value: 25000000</span><br><span class="line">&gt; &gt; --The threshold (in bytes) for the input file size of the small tables; if the file size is smaller than this threshold, it will try to convert the common join into map join.</span><br><span class="line">&gt; &gt; hive.mapjoin.smalltable.filesize=25mb</span><br><span class="line">&gt; &gt;</span><br></pre></td></tr></table></figure></blockquote><p>&gt;</p><blockquote><ul><li>手动指定使用map join</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; &gt; select /*+mapjoin(n)*/m.col,m.col2,n.col3</span><br><span class="line">&gt; &gt; from m join n on m.col=n.col;</span><br><span class="line">&gt; &gt;</span><br></pre></td></tr></table></figure></blockquote><p>&gt;</p><blockquote><p>map join的使用场景？</p><ul><li>关联操作中有一张表非常小</li><li>不等值的连接操作</li></ul></blockquote><p>分桶操作的好处？</p><ul><li><p>存在join查询时更高效</p></li><li><p>取样更高效</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt;   --分桶查询参数设置</span><br><span class="line">&gt;</span><br><span class="line">&gt;   --分桶取样</span><br><span class="line">&gt;   select * from bucket_user</span><br><span class="line">&gt;   tablesample(bucket 1 out of 2 on id);</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></li></ul></blockquote><p>&gt;<br>&gt;</p><blockquote><p><strong>distribute by 和sort by</strong></p><ul><li>distribute分散数据</li></ul><blockquote><p>distribute by col</p><p>按照col列把数据分散到不同的reduce</p></blockquote><ul><li>sort排序</li></ul><blockquote><p>sort by col</p><p>按照col列把数据排序</p></blockquote><ul><li>两者结合使用可以确保每个reduce的输出都是有序的，但不保证全局的有序性</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt; select col1,col2 from m</span><br><span class="line">&gt; distribute by col1</span><br><span class="line">&gt; sort by col1 asc,col2 desc;</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><p>&gt;</p><blockquote><p><strong>distribute by与group by</strong></p><ul><li><p>都是按照key划分数据</p></li><li><p>都使用了reduce操作</p></li><li><p>唯一的区别：</p><blockquote><p>distribute by单纯的分散数据</p><p>group by把相同key的数据聚集到一起，后续必须是聚合操作</p></blockquote></li></ul><p><strong>order by和sort by</strong></p><ul><li>order by是全局排序</li><li>sort by只确保每个reduce上输出的数据有序，如果只有一个reduce，那就和order by作用一样</li></ul><p><strong>distribute by和sort by的应用场景</strong></p><ul><li>map输出的文件大小不均</li><li>reduce输出的文件大小不均</li><li>小文件过多</li><li>文件超大</li></ul><p>hive只支持<strong>union all</strong>不支持<strong>union</strong></p><p><strong>union all使用注意：</strong></p><ul><li>union all连接的子表不允许有别名</li><li>合并字段的名字、类型、字段个数要一样</li><li>如果要从合并后的表中查数据，那合并的表必须有别名</li></ul><p><strong>一般的子查询需要别名</strong></p><p><strong>union all执行比较快，只有map任务，没有reduce任务</strong></p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt; --UDTF</span><br><span class="line">&gt; --一列转多行</span><br><span class="line">&gt; select id,adid</span><br><span class="line">&gt; from winfunc</span><br><span class="line">&gt; lateral view explode(split(type,'B')) tt as adid;</span><br><span class="line">&gt; --explode(array或map)</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><p>&gt;</p><blockquote><p>正则表达式的贪婪匹配及防止贪婪匹配</p><p>UDF：用户自定义函数</p><blockquote><p>针对单条记录</p><p>创建函数</p><ul><li>自定义一个java类</li><li>继承UDF类</li><li>重写evaluate方法</li><li>打jar包</li><li>hive执行add jar</li><li>hive执行创建模板函数</li><li>hql中使用</li></ul></blockquote><p>UDAF：用户自定义聚合函数</p><blockquote><p>自定义实现两步走：</p><ul><li>编写resolver类进行类型检查和操作符重载</li><li>编写evaluator类实现UDAF的逻辑</li></ul><p>mode表示了UDAF在mapreduce的各个阶段</p><ul><li>partial1（map）</li><li>partial2（combiner）</li><li>final（reduce）</li><li>complete（只有map没有reduce时执行）</li></ul><p>理解了mode的含义就理解了hive的UDAF运行流程</p><p>mapreduce阶段调用函数</p><ul><li>map</li></ul><blockquote><p>init()、iterate()、terminatePartial()</p></blockquote><ul><li>combiner</li></ul><blockquote><p>merge()、terminatePartial()</p></blockquote><ul><li>reduce</li></ul><blockquote><p>init()、merge()、terminate()</p></blockquote></blockquote></blockquote><p>##Hive SQL优化</p><blockquote><p>hive优化分为：</p><blockquote><p>表优化</p><p>sql优化</p><p>job优化</p><p>map优化</p><p>shuffle优化</p><p>reduce优化</p></blockquote><p>1.对于oracle等关系型数据库中的sql语句，既有join on又有where时，它会自动优化先执行where中的条件过滤之后再执行join on关联其他表；但是hive不同，<strong>hive会先执行join on关联表然后再执行where条件</strong></p><p>2.对于数据量比较大的情况下，count(distinct)优化</p><p>优化前：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; --执行一个job</span><br><span class="line">&gt; select count(distinct id) from tablename;</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><p>&gt;</p><blockquote><p>优化后：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt; --执行两个job</span><br><span class="line">&gt; select count(1) from (select distinct id from tablename) tmp;</span><br><span class="line">&gt; --或</span><br><span class="line">&gt; select count(1) from (select id from tablename group by id) tmp;</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><p>&gt;</p><blockquote><p>3.</p><p>优化前</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; select a,sum(b),count(distinct c),count(distinct d) from test group by a;</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><p>&gt;</p><blockquote><p>优化后</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&gt; select a,sum(b) as b,count(c) as c,count(d) as d from (</span><br><span class="line">&gt; select a,0 as b,c,null as d from test</span><br><span class="line">&gt; group by a,d</span><br><span class="line">&gt; union all</span><br><span class="line">&gt; select a,0 as b,null as c,d from test </span><br><span class="line">&gt; group by a,c</span><br><span class="line">&gt; union all</span><br><span class="line">&gt; select a,b,null as c,null as d from test</span><br><span class="line">&gt; ) tmp</span><br><span class="line">&gt; group by a;</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><p>&gt;</p><blockquote><p>4.Hive的优化目标</p><p>在有限的资源下高效执行。</p><blockquote><p>常见问题</p><ul><li>数据倾斜</li><li>Map数设置</li><li>Reduce数设置</li><li>其他</li></ul></blockquote><p>5.explain extended sql可以查看该hive sql的执行语法树，有几个stage，map阶段，reduce阶段，map阶段扫描的哪些数据等</p><p>6.job优化</p><p>压缩优化</p><p>压缩效果好的占用磁盘空间小，但是会消耗更多的CPU时间</p><p>对于中间job结果，选择轻度压缩(SnappyCodec)，最终结果可以选择重度压缩(GzipCodec)，压缩类型一般为block</p><p>7.map优化</p><p>文件大小太大，设置参数</p><p>小文件数量过多，设置参数在map阶段合并小文件</p><p>map端聚合即combiner</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; set hive.exec.aggr=true;</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><p>&gt;</p><blockquote><p>推测执行</p><p>shuffle优化配置参数（溢出比，压缩）</p><p>reduce优化</p><blockquote><p>需要reduce操作的查询</p><ul><li>聚合函数</li></ul><blockquote><p>sum,count,distinct…</p></blockquote><ul><li>高级查询</li></ul><blockquote><p>group by,join,distribute by,cluster by…</p><p>order by比较特殊，只需一个reduce</p></blockquote></blockquote><p>开启推测执行参数</p></blockquote><p>##hive中的哪些SQL会执行MR?</p><p>##hive多表查询的条件写在join里好还是用子查询？</p><blockquote><p>两个表关联查询，是把条件都写join on里好？<br>select a.<em> from a join b on ( a.id = b.id and a.a = 2 and b.c = 3)<br>还是写成子查询的形式比较快？<br>select a.</em> from a join (select b.* from b where b.c = 3) c on (a.id = c.id) where a.a = 2</p><p>或者hive其实两种形式都会优化，所以速度差不多？</p><p>分情况吧。</p><p>1、如果不存在数据倾斜，应该是第一种，从执行逻辑计划来看，这种方式会在reduce时候过滤掉不满足on 条件中的数据，而第二种方式在join之前会有一次数据读取处理过程，多一次磁盘IO过程，因此第一种方式时间短一些</p><p>2、若是join的两张表在关联时因笛卡儿积产生数据倾斜，第一种方式处理时间估计就比较久了，on条件过滤也是挽救不了，需要采取其他的方式进行优化</p><p>HQL编写时为了处理数据，在了解数据的基础上对语句优化更有方向</p></blockquote><p>##创建一个分区且排序的表(???????????????)</p><blockquote><p>create table stu_buck(Sno int,Sname string,Sex string,Sage int,Sdept string)<br>partitioned by (dt string,country string)<br>clustered by(Sno) into 3 buckets<br>row format delimited<br>fields terminated by ‘,’;</p><p>load data inpath ‘/hive0227/students.txt’ overwrite into table stu_buck partition(dt=’2018-02-27’,country=”china”);</p><p>insert overwrite table stu_buck partition(dt=’2018-02-27’,country=”china”)<br>select * from stu_buck;</p><p>==================<br>create table stu_buck1(Sno int,Sname string,Sex string,Sage int,Sdept string)<br>partitioned by (dt string,country string)<br>clustered by(Sno) into 3 buckets<br>row format delimited<br>fields terminated by ‘,’;</p><p>insert overwrite table stu_buck1 partition(dt=’2018-02-27’,country=”china”)<br>select Sno,Sname,Sex,Sage,Sdept from stu_buck;<br>最终生成1个文件（只有map阶段）</p><p>insert overwrite table stu_buck1 partition(dt=’2018-02-27’,country=”china”)<br>select Sno,Sname,Sex,Sage,Sdept from stu_buck cluster by(sno);<br>最终生成三个文件（两个阶段（一次mapreduce+一次mapreduce））<br>两个阶段的maptask数与reducetask数不一样</p><p>select * from stu_buck1 tablesample(bucket 1 out of 3 on sno);</p><p>set mapreduce.job.reduces=3;<br>insert overwrite table stu_buck1 partition(dt=’2018-02-27’,country=”china”)<br>select Sno,Sname,Sex,Sage,Sdept from stu_buck cluster by(sno);<br>最终生成三个文件（两个阶段（一次mapreduce+一次mapreduce））<br>两个阶段的maptask数与reducetask数不一样</p><p>set mapreduce.job.reduces=2;<br>insert overwrite table stu_buck1 partition(dt=’2018-02-27’,country=”china”)<br>select Sno,Sname,Sex,Sage,Sdept from stu_buck cluster by(sno);<br>最终生成三个文件（两个阶段（一次mapreduce+一次mapreduce））<br>两个阶段的maptask数与reducetask数不一样</p></blockquote><p>在hive命令行查看hdfs上的文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">hive通过dfs命令查看hdfs文件</span></span><br><span class="line">0: jdbc:hive2://node01:10000&gt; dfs -ls /;</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2://node01:10000&gt; dfs -ls /user/hive/warehouse/itheima.db/stu_buck1/dt=2018-02-27/country=china;</span><br><span class="line">+--------------------------------------------------------------------------------------------------------------------------------------------+--+</span><br><span class="line">|                                                                 DFS Output                                                                 |</span><br><span class="line">+--------------------------------------------------------------------------------------------------------------------------------------------+--+</span><br><span class="line">| Found 3 items                                                                                                                              |</span><br><span class="line">| -rwxr-xr-x   2 root supergroup        193 2018-02-27 08:12 /user/hive/warehouse/itheima.db/stu_buck1/dt=2018-02-27/country=china/000000_0  |</span><br><span class="line">| -rwxr-xr-x   2 root supergroup        170 2018-02-27 08:12 /user/hive/warehouse/itheima.db/stu_buck1/dt=2018-02-27/country=china/000001_0  |</span><br><span class="line">| -rwxr-xr-x   2 root supergroup        164 2018-02-27 08:12 /user/hive/warehouse/itheima.db/stu_buck1/dt=2018-02-27/country=china/000002_0  |</span><br><span class="line">+--------------------------------------------------------------------------------------------------------------------------------------------+--+</span><br><span class="line">4 rows selected (0.023 seconds)</span><br></pre></td></tr></table></figure><p>##hive之本地模式</p><blockquote><p>hive的本地模式默认是不开启的</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash"> <span class="comment">#通过设置参数开启本地模式</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> <span class="built_in">set</span> hive.exec.mode.local.auto=<span class="literal">true</span>;</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"></span></span><br></pre></td></tr></table></figure></blockquote><p>&gt;</p><blockquote><p>是否自动开启hive的本地执行模式：就是以进程模拟的形式模拟出一套MR执行的环境<br>是否自动开启的是有条件的：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash"> <span class="comment">#1.job的输入数据大小必须小于参数：</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> <span class="comment">#(默认128MB)数据量小</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> hive.exec.mode.local.auto.inputbytes.max</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> <span class="comment">#2.job的map数必须小于参数：</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> <span class="comment">#(默认4)</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> hive.exec.mode.local.auto.tasks.max</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> <span class="comment">#3.job的reduce数必须为0或者1</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"></span></span><br></pre></td></tr></table></figure><p>hive会根据每次任务自动判断是否启用本地模式；如果不满足hive还是跑集群模式（把组装好的mr程序提交给yarn）</p><p>本地模式只是在本地测试为了方便和高效测试而开启的，在生产集群环境上禁止开启</p></blockquote><p>Number of reduce tasks not specified. Estimated from input data size: 1<br>INFO  : In order to change the average load for a reducer (in bytes):<br>INFO  :   set hive.exec.reducers.bytes.per.reducer=<number><br>INFO  : In order to limit the maximum number of reducers:<br>INFO  :   set hive.exec.reducers.max=<number><br>INFO  : In order to set a constant number of reducers:<br>INFO  :   set mapreduce.job.reduces=<number></number></number></number></p><p>0: jdbc:hive2://node01:10000&gt; select * from stu_buck1 sort by(sage);<br>INFO  : Number of reduce tasks not specified. Estimated from input data size: 1<br>INFO  : In order to change the average load for a reducer (in bytes):<br>INFO  :   set hive.exec.reducers.bytes.per.reducer=<number><br>INFO  : In order to limit the maximum number of reducers:<br>INFO  :   set hive.exec.reducers.max=<number><br>INFO  : In order to set a constant number of reducers:<br>INFO  :   set mapreduce.job.reduces=<number><br>INFO  : number of splits:1<br>INFO  : Submitting tokens for job: job_1519711328851_0001<br>INFO  : The url to track the job: <a href="http://node01:8088/proxy/application_1519711328851_0001/" target="_blank" rel="noopener">http://node01:8088/proxy/application_1519711328851_0001/</a><br>INFO  : Starting Job = job_1519711328851_0001, Tracking URL = <a href="http://node01:8088/proxy/application_1519711328851_0001/" target="_blank" rel="noopener">http://node01:8088/proxy/application_1519711328851_0001/</a><br>INFO  : Kill Command = /export/servers/hadoop/bin/hadoop job  -kill job_1519711328851_0001<br>INFO  : Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1<br>INFO  : 2018-02-27 14:03:02,701 Stage-1 map = 0%,  reduce = 0%<br>INFO  : 2018-02-27 14:03:18,322 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.22 sec<br>INFO  : 2018-02-27 14:03:31,264 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 4.75 sec<br>INFO  : MapReduce Total cumulative CPU time: 4 seconds 750 msec<br>INFO  : Ended Job = job_1519711328851_0001</number></number></number></p><p>create table stu_buck000(Sno int,Sname string,Sex string,Sage int,Sdept string)<br>clustered by(Sno)<br>sorted by(Sno DESC)<br>into 4 buckets<br>row format delimited<br>fields terminated by ‘,’;</p><p>hiveSQL与SQL格式化日期</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--hive SQL</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">DATE_FORMAT</span>(<span class="keyword">current_timestamp</span>(),<span class="string">'yyyy-MM-dd HH:mm:ss'</span>) <span class="keyword">AS</span> dates</span><br><span class="line"><span class="keyword">FROM</span> dual; </span><br><span class="line"><span class="comment">--2018-02-27 15:25:05</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">DATE_FORMAT</span>(<span class="keyword">current_timestamp</span>(),<span class="string">'y-MM-d H:m:s'</span>) <span class="keyword">AS</span> dates</span><br><span class="line"><span class="keyword">FROM</span> dual; </span><br><span class="line"><span class="comment">--2018-02-27 15:25:42</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">--SQL</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">name</span>, <span class="keyword">url</span>, <span class="keyword">DATE_FORMAT</span>(<span class="keyword">Now</span>(),<span class="string">'%Y-%m-%d'</span>) <span class="keyword">AS</span> dates</span><br><span class="line"><span class="keyword">FROM</span> Websites; </span><br><span class="line"><span class="comment">--2018-02-27</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;#hive&lt;/p&gt;
&lt;h2 id=&quot;安装&quot;&gt;&lt;a href=&quot;#安装&quot; class=&quot;headerlink&quot; title=&quot;安装&quot;&gt;&lt;/a&gt;安装&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;自行下载安装包&lt;/li&gt;
&lt;li&gt;上传&lt;strong&gt;hive-1.2.1.tar.gz&lt;/stron
      
    
    </summary>
    
      <category term="bigdata" scheme="http://www.bluedreams.top/categories/bigdata/"/>
    
    
      <category term="bigdata" scheme="http://www.bluedreams.top/tags/bigdata/"/>
    
  </entry>
  
  <entry>
    <title>术语解释</title>
    <link href="http://www.bluedreams.top/2018/03/12/term-interpretation/"/>
    <id>http://www.bluedreams.top/2018/03/12/term-interpretation/</id>
    <published>2018-03-11T22:37:43.900Z</published>
    <updated>2018-03-11T22:46:12.053Z</updated>
    
    <content type="html"><![CDATA[<p>#术语解释</p><h5 id="主从"><a href="#主从" class="headerlink" title="主从"></a>主从</h5><h5 id="主备"><a href="#主备" class="headerlink" title="主备"></a>主备</h5><h5 id="读写分离-database"><a href="#读写分离-database" class="headerlink" title="读写分离(database)"></a>读写分离(database)</h5><h5 id="水平扩展vs垂直扩展"><a href="#水平扩展vs垂直扩展" class="headerlink" title="水平扩展vs垂直扩展"></a>水平扩展vs垂直扩展</h5><ul><li>水平扩展(横向扩展)<ul><li>添加组件</li></ul></li><li>垂直扩展(纵向扩展)<ul><li>提高自身能力</li></ul></li></ul><blockquote><p>case：</p><ul><li>添加多台服务器与提高单台服务器运算存储能力</li><li>卡车从A到B运输木材</li><li>团队与个人</li><li>三个臭皮匠与一个诸葛亮</li><li>java，前端，DBA，架构师<strong>vs</strong>全栈工程师</li></ul></blockquote><p><a href="https://www.jianshu.com/p/86c2620d34aa" target="_blank" rel="noopener">参考</a></p><p>CAP理论</p><p>Consistency一致性、Availability可用性、Partition-tolerance分区可容忍性</p><p>ACID</p><p>关系型数据库的事务操作遵循ACID原则</p><p>指在写入/异动资料的过程中，为保证交易正确可靠所必须具备的四个特性</p><p>原子性（Atomicity，或称不可分割性）、一致性（Consistency）、隔离性（Isolation，又称独立性）和持久性（Durability）</p><p>BASE</p><p>CAS</p><h5 id="浅析数据一致性"><a href="#浅析数据一致性" class="headerlink" title="浅析数据一致性"></a>浅析数据一致性</h5><ul><li>数据一致性模型</li></ul><blockquote><p>一些分布式系统通过复制数据来提高系统的可靠性和容错性，并且将数据的不同的副本存放在不同的机器，由于维护数据副本的一致性代价高，因此许多系统采用弱一致性来提高性能，一些不同的一致性模型也相继被提出。</p><ol><li>强一致性： 要求无论更新操作是在哪一个副本执行，之后所有的读操作都要能获得最新的数据。</li><li>弱一致性：用户读到某一操作对系统特定数据的更新需要一段时间，我们称这段时间为“不一致性窗口”。</li><li>最终一致性：是弱一致性的一种特例，保证用户最终能够读取到某操作对系统特定数据的更新。</li></ol></blockquote><p><a href="http://www.importnew.com/20633.html" target="_blank" rel="noopener">浅析数据一致性</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;#术语解释&lt;/p&gt;
&lt;h5 id=&quot;主从&quot;&gt;&lt;a href=&quot;#主从&quot; class=&quot;headerlink&quot; title=&quot;主从&quot;&gt;&lt;/a&gt;主从&lt;/h5&gt;&lt;h5 id=&quot;主备&quot;&gt;&lt;a href=&quot;#主备&quot; class=&quot;headerlink&quot; title=&quot;主备&quot;&gt;&lt;/a&gt;
      
    
    </summary>
    
      <category term="bigdata" scheme="http://www.bluedreams.top/categories/bigdata/"/>
    
    
      <category term="bigdata" scheme="http://www.bluedreams.top/tags/bigdata/"/>
    
  </entry>
  
  <entry>
    <title>HDFS浅析</title>
    <link href="http://www.bluedreams.top/2018/03/12/hadoop-hdfs/"/>
    <id>http://www.bluedreams.top/2018/03/12/hadoop-hdfs/</id>
    <published>2018-03-11T22:37:43.875Z</published>
    <updated>2018-03-11T22:44:51.692Z</updated>
    
    <content type="html"><![CDATA[<h1 id="HDFS浅析"><a href="#HDFS浅析" class="headerlink" title="HDFS浅析"></a>HDFS浅析</h1><h2 id="HDFS-基本概念"><a href="#HDFS-基本概念" class="headerlink" title="HDFS 基本概念"></a>HDFS 基本概念</h2><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><ul><li>Hadoop Distribute File System，即Hadoop分布式文件系统</li><li>Hadoop的核心组件之一</li><li>作为最底层的分布式存储服务而存在</li><li>主要解决大数据的存储问题</li><li>应用前景广泛，为存储和处理大规模数据提供所需的扩展能力</li></ul><h3 id="设计目标"><a href="#设计目标" class="headerlink" title="设计目标"></a>设计目标</h3><ul><li>硬件故障</li></ul><blockquote><p>故障检测和自动快速恢复</p></blockquote><ul><li>流数据访问</li></ul><blockquote><p>更多是为了批量处理，而非用户交互</p><p>重点在数据访问的高吞吐量，而不是数据访问的低延迟</p></blockquote><ul><li>大型数据集</li></ul><blockquote><p>支持大文件</p></blockquote><ul><li>简单一致性模型</li></ul><blockquote><p>write-once-read-many</p></blockquote><ul><li>移动计算比移动数据更便宜</li></ul><blockquote><p>数据不动计算移动</p></blockquote><ul><li>在异构硬件和软件平台上的可移植性</li></ul><blockquote><p>可移植性</p></blockquote><h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><ul><li>不能做到低延迟</li></ul><blockquote><p>hdfs将重点放在了数据访问的高吞吐量，从而牺牲了低延迟。</p><p>对于数据访问低延迟的需求，HBase是更好的选择。</p></blockquote><ul><li>不适合大量小文件存储</li></ul><blockquote><p>由于namenode将文件系统的元数据存储在内存中，因此该文件系统能存储的文件受限于namenode的内存容量，根据经验，每个文件、目录和数据块的存储信息大约占150字节。因此，如果大量的小文件存储，每个小文件会占一个数据块，会使用大量的内存，有可能超过当前硬件的能力。</p></blockquote><ul><li>不适合多用户写入文件，修改文件</li></ul><blockquote><blockquote><p>Hadoop2.0虽然支持文件的追加功能，但是还是不建议对HDFS上的 文件进行修改，因为效率低。</p><p>对于上传到HDFS上的文件，不支持修改文件，HDFS适合一次写入，多次读取的场景。</p><p>HDFS不支持多用户同时执行写操作，即同一时间，只能有一个用户执行写操作。</p></blockquote></blockquote><h3 id="重要特性"><a href="#重要特性" class="headerlink" title="重要特性"></a>重要特性</h3><ul><li>文件系统</li></ul><blockquote><p>用于存储文件</p><p>通过统一命名空间目录树定位文件</p></blockquote><ul><li>分布式</li></ul><blockquote><p>由很多服务器联合起来实现其功能，集群中的服务器有各自的角色</p></blockquote><h4 id="master-slave架构"><a href="#master-slave架构" class="headerlink" title="master/slave架构"></a>master/slave架构</h4><ul><li>HDFS采用master/slave架构。</li><li>一般一个HDFS集群是有一个namenode和一定数目的datanode组成。</li><li>namenode是HDFS集群的主节点，datanode是HDFS集群的从节点，两种角色各司其职，共同协调完成分布式文件存储服务。</li></ul><h4 id="分块存储"><a href="#分块存储" class="headerlink" title="分块存储"></a>分块存储</h4><ul><li>HDFS中的文件在物理上是分块（block）存储的。</li><li>块大小通常可以通过配置参数来规定，默认大小在hadoop2.x版本中是128M，在hadoop1.x版本中是64M。</li><li>参数设置</li></ul><blockquote><p>设置每个namenode节点和datanode节点的<strong>hdfs-site.xml</strong>文件，将文件中的dfs.block.size属性值改为33554432（32M）单位大小为比特（B），重启集群。</p><table><thead><tr><th>name</th><th>value</th><th>description</th></tr></thead><tbody><tr><td>dfs.blocksize</td><td>134217728</td><td>The default block size for new files, in bytes. You can use the following suffix (case insensitive): k(kilo), m(mega), g(giga), t(tera), p(peta), e(exa) to specify the size (such as 128k, 512m, 1g, etc.), Or provide complete size in bytes (such as 134217728 for 128 MB).</td></tr></tbody></table><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">&gt; # hadoop1.x</span><br><span class="line">&gt; <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">&gt;   <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">&gt;     <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.block.size<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">&gt;     <span class="tag">&lt;<span class="name">value</span>&gt;</span>33554432<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">&gt;   <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">&gt; <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">&gt; # hadoop2.x中128M</span><br><span class="line">&gt; <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">&gt;   <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">&gt;     <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.blocksize<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">&gt;     <span class="tag">&lt;<span class="name">value</span>&gt;</span>134217728<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">&gt;   <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">&gt; <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><h4 id="命名空间（namespace）"><a href="#命名空间（namespace）" class="headerlink" title="命名空间（namespace）"></a>命名空间（namespace）</h4><ul><li>HDFS支持传统的层次型文件组织结构。用户或者应用程序可以创建目录，然后将文件保存在这些目录里。文件系统名字空间的层次结构和大多数现有的文件系统类似：用户可以创建、删除、移动或重命名文件。</li><li>namenode负责维护文件系统的名字空间，任何对文件系统名字空间或属性的修改都将被namenode记录下来。</li><li>HDFS会给客户端提供一个统一的抽象目录树，客户端通过路径来访问文件，形如：hdfs://namenode:port/dir-a/dir-b/dir-c/file.data。</li></ul><h4 id="namenode元数据管理"><a href="#namenode元数据管理" class="headerlink" title="namenode元数据管理"></a>namenode元数据管理</h4><ul><li>我们把目录结构及文件分块位置信息叫做元数据。</li><li>Namenode 负责维护整个hdfs文件系统的目录树结构，以及每一个文件所对应的block块信息（block的id及所在的datanode服务器）。</li></ul><h4 id="datanode数据存储"><a href="#datanode数据存储" class="headerlink" title="datanode数据存储"></a>datanode数据存储</h4><ul><li>文件的各个block的具体存储管理由datanode节点承担。</li><li>每一个block都可以在多个datanode上。</li><li>Datanode需要定时向Namenode汇报自己持有的block信息。</li><li>存储多个副本（副本数量也可以通过参数设置dfs.replication，默认是3）。</li></ul><h4 id="副本机制"><a href="#副本机制" class="headerlink" title="副本机制"></a>副本机制</h4><ul><li>为了容错，文件的所有block都会有副本。每个文件的block大小和副本系数都是可配置的。应用程序可以指定某个文件的副本数目。副本系数可以在文件创建的时候指定，也可以在之后改变。</li></ul><h4 id="一次写入，多次读出"><a href="#一次写入，多次读出" class="headerlink" title="一次写入，多次读出"></a>一次写入，多次读出</h4><ul><li>HDFS是设计成适应一次写入，多次读出的场景，且不支持文件的修改。</li><li>正因为如此，HDFS 适合用来做大数据分析的底层存储服务，并不适合用来做网盘等应用，因为修改不方便，延迟大，网络开销大，成本太高。</li></ul><blockquote><p>HDFS设置block的目的</p><p><a href="https://www.cnblogs.com/Dhouse/p/6901028.html?utm_source=itdadao&amp;utm_medium=referral" target="_blank" rel="noopener">参考链接</a></p><p>为什么64MB(或128MB或256MB)是最优选择？</p><p>为什么不能远少于64MB(或128MB或256MB) （普通文件系统的数据块大小一般为4KB）</p><ul><li>1.减少硬盘寻道时间</li></ul><blockquote><p>HDFS设计前提是支持大容量的流式数据操作，所以即使是一般的数据读写操作，涉及到的数据量都是比较大的。假如数据块设置过少，那需要读取的数据块就比较多，由于数据块在硬盘上非连续存储，普通硬盘因为需要移动磁头，所以随机寻址较慢，读越多的数据块就增大了总的硬盘寻道时间。当硬盘寻道时间比io时间还要长的多时，那么硬盘寻道时间就成了系统的一个瓶颈。合适的块大小有助于减少硬盘寻道时间，提高系统吞吐量。</p></blockquote><ul><li>2.减少Namenode内存消耗</li></ul><blockquote><p>对于HDFS，他只有一个Namenode节点，他的内存相对于Datanode来说，是极其有限的。然而，namenode需要在其内存FSImage文件中中记录在Datanode中的数据块信息，假如数据块大小设置过少，而需要维护的数据块信息就会过多，那Namenode的内存可能就会伤不起了。</p></blockquote><p>为什么不能远大于64MB(或128MB或256MB)？</p><p>这里主要从上层的MapReduce框架来讨论</p></blockquote><h2 id="基本操作"><a href="#基本操作" class="headerlink" title="基本操作"></a>基本操作</h2><h2 id="基本原理"><a href="#基本原理" class="headerlink" title="基本原理"></a>基本原理</h2><p>NameNode概述</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;HDFS浅析&quot;&gt;&lt;a href=&quot;#HDFS浅析&quot; class=&quot;headerlink&quot; title=&quot;HDFS浅析&quot;&gt;&lt;/a&gt;HDFS浅析&lt;/h1&gt;&lt;h2 id=&quot;HDFS-基本概念&quot;&gt;&lt;a href=&quot;#HDFS-基本概念&quot; class=&quot;headerlink
      
    
    </summary>
    
      <category term="bigdata" scheme="http://www.bluedreams.top/categories/bigdata/"/>
    
    
      <category term="bigdata" scheme="http://www.bluedreams.top/tags/bigdata/"/>
    
  </entry>
  
  <entry>
    <title>hadoop专题</title>
    <link href="http://www.bluedreams.top/2018/03/12/hadoop/"/>
    <id>http://www.bluedreams.top/2018/03/12/hadoop/</id>
    <published>2018-03-11T22:37:43.861Z</published>
    <updated>2018-03-11T22:44:21.578Z</updated>
    
    <content type="html"><![CDATA[<p>#hadoop专题</p><p>网络同步集群时钟</p><p>yum install ntpdate</p><p>ntpdate cn.pool.ntp.org</p><p>查看java的安装位置which java</p><p>hadoop版本</p><p>官方提供的x86编译版本</p><p>如果需要安装x64版本或者适合于自己系统的版本，需要自己编译</p><p>修改hadoop的配置文件</p><p>hadoop-env.sh</p><p>指定java JDK的位置</p><p>core-site.xml</p><p>指定hdfs所使用的文件系统和hdfs的老大namenode节点</p><p>hdfs-site.xml</p><p>指定hdfs副本（这里的副本数包括自己）数量，默认是3</p><p>mapred-site.xml</p><blockquote><p>hadoop2.7.4的目录结构</p><p>/export/servers/hadoop/</p><p>bin<br>etc<br>include<br>lib<br>libexec<br>LICENSE.txt<br>logs<br>NOTICE.txt<br>README.txt<br>sbin<br>share</p><p>相关jar包</p><p>/export/servers/hadoop/share/hadoop</p><p>common<br>hdfs<br>httpfs<br>kms<br>mapreduce<br>tools<br>yarn</p><p>hadoop相关配置文件</p><p>/export/servers/hadoop/etc/hadoop</p><p>capacity-scheduler.xml<br>configuration.xsl<br>container-executor.cfg<br>core-site.xml<br>hadoop-env.cmd<br>hadoop-env.sh<br>hadoop-metrics2.properties<br>hadoop-metrics.properties<br>hadoop-policy.xml<br>hdfs-site.xml<br>httpfs-env.sh<br>httpfs-log4j.properties<br>httpfs-signature.secret<br>httpfs-site.xml<br>kms-acls.xml<br>kms-env.sh<br>kms-log4j.properties<br>kms-site.xml<br>log4j.properties<br>mapred-env.cmd<br>mapred-env.sh<br>mapred-queues.xml.template<br>mapred-site.xml<br>slaves<br>ssl-client.xml.example<br>ssl-server.xml.example<br>yarn-env.cmd<br>yarn-env.sh<br>yarn-site.xml</p><p>hadoop相关运行脚本</p><p>/export/servers/hadoop/sbin/</p><p>distribute-exclude.sh<br>hadoop-daemon.sh<br>hadoop-daemons.sh<br>hdfs-config.cmd<br>hdfs-config.sh<br>httpfs.sh<br>kms.sh<br>mr-jobhistory-daemon.sh<br>refresh-namenodes.sh<br>slaves.sh<br>start-all.cmd<br>start-all.sh<br>start-balancer.sh<br>start-dfs.cmd<br>start-dfs.sh<br>start-secure-dns.sh<br>start-yarn.cmd<br>start-yarn.sh<br>stop-all.cmd<br>stop-all.sh<br>stop-balancer.sh<br>stop-dfs.cmd<br>stop-dfs.sh<br>stop-secure-dns.sh<br>stop-yarn.cmd<br>stop-yarn.sh<br>yarn-daemon.sh<br>yarn-daemons.sh</p><p>hadoop的其他运行脚本</p><p>/export/servers/hadoop/bin/</p><p>container-executor<br>hadoop<br>hadoop.cmd<br>hdfs<br>hdfs.cmd<br>mapred<br>mapred.cmd<br>rcc<br>test-container-executor<br>yarn<br>yarn.cmd</p></blockquote><p>hadoop集群启动</p><p>需要启动HDFS和YARN两个集群</p><p>首次启动HDFS集群需要进行格式化（初始化）</p><blockquote><p>因为HDFS在物理上是不存在的，需要一些清理和准备工作</p><p>执行命令</p><p>hadoop1.x</p><figure class="highlight plain"><figcaption><span>namenode –format```</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt;</span><br><span class="line">&gt; 或者</span><br><span class="line">&gt;</span><br><span class="line">&gt; hadoop2.x</span><br><span class="line">&gt;</span><br><span class="line">&gt; ```hadoop namenode –format</span><br></pre></td></tr></table></figure></blockquote><p>方式一：</p><blockquote><p>hdfs集群启动与yarn集群启动</p><p>start-dfs.sh</p><p>start-yarn.sh</p><p>stop-dfs.sh</p><p>stop-yarn.sh</p></blockquote><p>方式二：</p><blockquote><p>hdfs与yarn集群一键启动</p><p>start-all.sh</p><p>stop-all.sh</p></blockquote><p>方式三：</p><blockquote><p>单节点启动</p><p>hadoop-daemon.sh start namenode</p><p>hadoop-daemon.sh start datanode</p><p>hadoop-daemon.sh start secondarynamenode</p><p>yarn-daemon.sh start resourcemanager</p><p>yarn-daemon.sh start nodemanager</p></blockquote><p>jps查看正在运行的java进程</p><p>NameNode</p><p>DataNode</p><p>SencondaryNameNode</p><p>ResourceManager</p><p>NodeManager</p><p>通过Web-UI查看</p><p>浏览器访问hdfs集群NameNode地址=<a href="http://node01:50070/" target="_blank" rel="noopener">http://node01:50070/</a></p><p>浏览器访问yarn集群ResourceManager地址=<a href="http://node01:8088/" target="_blank" rel="noopener">http://node01:8088/</a></p><p>单点故障</p><p>Hadoop初体验</p><p>HDFS的使用</p><p>从Linux本地上传一个文本文件到hdfs的/test/input目录下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -mkdir -p /test/input</span><br><span class="line">hadoop fs -put /root/words.txt /test/input</span><br></pre></td></tr></table></figure><p>运行mapreduce程序</p><p>在hadoop安装包hadoop2.7.4/share/hadoop/mapreduce目录下有hadoop-mapreduce-examples-2.7.4.jar</p><p>计算圆周率PI</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">蒙特卡罗方法估算圆周率</span></span><br><span class="line">hadoop jar /export/server/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.4.jar pi 20 50</span><br></pre></td></tr></table></figure><p>HDFS Shell客户端操作</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">hadoop1.x</span></span><br><span class="line"><span class="meta">#</span><span class="bash">hdfs dfs &lt;args&gt;</span></span><br><span class="line"><span class="meta">#</span><span class="bash">hadoop2.x</span></span><br><span class="line"><span class="meta">#</span><span class="bash">hadoop fs &lt;args&gt;</span></span><br><span class="line"></span><br><span class="line">hadoop fs -ls hdfs://node01:9000/</span><br><span class="line">hadoop fs -ls file:///</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;#hadoop专题&lt;/p&gt;
&lt;p&gt;网络同步集群时钟&lt;/p&gt;
&lt;p&gt;yum install ntpdate&lt;/p&gt;
&lt;p&gt;ntpdate cn.pool.ntp.org&lt;/p&gt;
&lt;p&gt;查看java的安装位置which java&lt;/p&gt;
&lt;p&gt;hadoop版本&lt;/p&gt;
&lt;p&gt;官
      
    
    </summary>
    
      <category term="bigdata" scheme="http://www.bluedreams.top/categories/bigdata/"/>
    
    
      <category term="bigdata" scheme="http://www.bluedreams.top/tags/bigdata/"/>
    
  </entry>
  
  <entry>
    <title>分布式集群</title>
    <link href="http://www.bluedreams.top/2018/03/12/distributed-software-install/"/>
    <id>http://www.bluedreams.top/2018/03/12/distributed-software-install/</id>
    <published>2018-03-11T22:37:43.808Z</published>
    <updated>2018-03-11T22:42:57.871Z</updated>
    
    <content type="html"><![CDATA[<p>#分布式集群</p><p>###前置事项</p><ul><li><p>规划集群</p></li><li><p>Linux存储、系统及版本选择</p><ul><li>Ubuntu16.x</li><li>Centos6.x</li><li>主节点一般选大内存，小存储<ul><li>主节点一般比较消耗内存</li><li>一般32G/64G,奢侈128G/256G</li><li>硬盘可是视情况选择</li></ul></li><li>数据节点一般选择大存储，小内存<ul><li>数据节点一般消耗磁盘存储空间</li><li>一般是2T/4T</li><li>内存4G/8G</li></ul></li></ul></li><li><p>安装系统</p><ul><li>阿里云选择镜像或者自建集群系统盘安装系统</li><li>KVM虚拟化技术</li><li>虚拟机安装（VM or Vbox）<ul><li>网络NAT</li><li>配置网关，注意IP地址</li><li>可以选择安装单台或者安装完一台后复制多份（修改）</li></ul></li></ul></li><li><p>安装节点系统</p><ul><li><p>安装完成后注意事项</p><ul><li><p>修改主机名</p><ul><li>临时</li></ul></li></ul></li></ul></li></ul><pre><code>  <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hostname vm01</span><br></pre></td></tr></table></figure>  - 永久  <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/sysconfig/network</span><br></pre></td></tr></table></figure>  &gt; 将<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">  &gt;</span><br><span class="line">  &gt; hosts文件修改后立刻生效</span><br><span class="line"></span><br><span class="line">  ​</span><br><span class="line"></span><br><span class="line">- 修改MAC地址</span><br><span class="line"></span><br><span class="line">  - 方式1</span><br><span class="line"></span><br><span class="line">    - 如果复制centos</span><br><span class="line">    - 启动后选择**我已复制**</span><br><span class="line">    - 删掉```rm -rf /etc/udev/rules.d/70-persistent-net.rules```文件</span><br><span class="line">    - 重启后会自动生成对应新文件</span><br><span class="line">    - 查看```cat /etc/udev/rules.d/70-persistent-net.rules```文件的MAC地址并记录</span><br><span class="line">    - 打开```vim /etc/sysconfig/network-scripts/ifcfg-eth0```修改__HWADDR__，改为刚刚记录下来的MAC地址值</span><br><span class="line">    - 重启网络服务或重启该节点服务器生效</span><br><span class="line"></span><br><span class="line">    ```shell</span><br><span class="line">    # 重启节点主机</span><br><span class="line">    halt</span><br><span class="line">    shutdown -r now</span><br><span class="line">    # 重启网络服务</span><br><span class="line">    service network stop</span><br><span class="line">    service network start</span><br><span class="line">    service network restart</span><br></pre></td></tr></table></figure>  - 方式2    - vmware设置-网络适配器-高级-生成MAC地址（多生成几次，以免重复）    - 修改MAC地址<figure class="highlight vim"><figcaption><span>/etc/udev/rules.d/70-persistent-net.rules```为上面生成的值并删掉多余的eth1</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">    </span><br><span class="line">  或者</span><br><span class="line">    </span><br><span class="line">- 修改```<span class="keyword">vim</span> /etc/udev/rules.d/<span class="number">70</span>-persistent-net.rules```文件，删掉eth0相关的配置，将eth1为eth0即可</span><br><span class="line">    </span><br><span class="line">- 修改```<span class="keyword">vim</span> /etc/sysconfig/network-scripts/ifcfg-eth0</span><br></pre></td></tr></table></figure>    - 将HWADDR值改成最新生成的MAC地址    - 立即重启<figure class="highlight plain"><figcaption><span>-r now```</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">    &gt; MAC地址样例</span><br><span class="line">    &gt;</span><br><span class="line">    &gt; 00:0C:29:CB:E8:AA</span><br><span class="line">    &gt; 00:0C:29:DA:6B:C9</span><br><span class="line"></span><br><span class="line">- 修改ifcfg-eth0</span><br><span class="line"></span><br><span class="line">  ```shell</span><br><span class="line">  vim /etc/sysconfig/network-scripts/ifcfg-eth0</span><br><span class="line">  #修改</span><br><span class="line">  DEVICE=&quot;eth0&quot;</span><br><span class="line">  HWADDR=&quot;00:0C:29:FD:FF:2A&quot;</span><br><span class="line">  ONBOOT=&quot;yes&quot;</span><br><span class="line">  IPADDR=192.168.1.31</span><br><span class="line">  NETMASK=255.255.255.0</span><br><span class="line">  GATEWAY=192.168.1.1</span><br><span class="line">  BOOTPROTO=static</span><br></pre></td></tr></table></figure>- 网络接口配置文件说明  <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">TYPE=Ethernet       #网卡类型</span><br><span class="line">DEVICE=eth0         #网卡接口名称</span><br><span class="line">ONBOOT=yes          #系统启动时是否自动加载</span><br><span class="line">BOOTPROTO=static    #启用地址协议 --static:静态协议 --bootp协议 --dhcp协议</span><br><span class="line">IPADDR=192.168.1.11      #网卡IP地址</span><br><span class="line">NETMASK=255.255.255.0    #网卡网络地址</span><br><span class="line">GATEWAY=192.168.1.1      #网卡网关地址</span><br><span class="line">DNS1=10.203.104.41       #网卡DNS地址</span><br><span class="line">HWADDR=00:0C:29:13:5D:74 #网卡设备MAC地址</span><br><span class="line">BROADCAST=192.168.1.255  #网卡广播地址</span><br></pre></td></tr></table></figure>- 重启网卡  <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">/etc/init.d/network stop</span><br><span class="line">/etc/init.d/network start</span><br></pre></td></tr></table></figure>- 网络服务的启动与关闭  - 方式1    <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">service network stop #关闭网络服务</span><br><span class="line">service network start #启动网络服务</span><br><span class="line">service network restart #重启网络服务</span><br></pre></td></tr></table></figure>  - 方式2    <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">/etc/init.d/network stop</span><br><span class="line">/etc/init.d/network start</span><br><span class="line">/etc/init.d/network restart</span><br></pre></td></tr></table></figure>- 网卡接口关闭与激活  <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ifdown eth0 #关闭网络</span><br><span class="line">ifup eth0 #启动网络</span><br></pre></td></tr></table></figure>- 网卡状态查询  <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service network status</span><br></pre></td></tr></table></figure>- 设置相同时区并永久同步相同时钟  <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> 1.修改时区为Asia/Shanghai</span><br><span class="line">cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime</span><br><span class="line"><span class="meta">#</span> 2.安装ntp服务器</span><br><span class="line">yum install ntp</span><br><span class="line"><span class="meta">#</span> 3.同步时间命令</span><br><span class="line">ntpdate cn.pool.ntp.org</span><br></pre></td></tr></table></figure>  - [参考01](http://blog.csdn.net/testcs_dn/article/details/39803919)  - [参考02](http://blog.csdn.net/wangmj518/article/details/49683305)</code></pre><ul><li><p>永久关闭放火墙</p><ul><li><p>服务器一般对外网开放某些端口，对内网永久关闭防火墙</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> 临时关闭和永久关闭防火墙</span><br><span class="line">service iptables stop &amp;&amp; chkconfig iptables off</span><br></pre></td></tr></table></figure></li></ul></li><li><p>配置免密登录</p><ul><li>一般是配置主节点到自己及其他从节点的免密登录</li><li>也可以配置集群中所有节点自己与自己及其他节点的免密登录</li></ul></li><li><p>安装一些软件包</p></li><li><p>配置YUM源</p></li></ul><blockquote><p>集群搭建需要注意的两点是：</p><p>1.<strong>关闭内网防火墙</strong></p><p>2.<strong>时间一致性</strong></p></blockquote><h3 id="windows-Linux-OS是hosts文件"><a href="#windows-Linux-OS是hosts文件" class="headerlink" title="windows/Linux OS是hosts文件"></a>windows/Linux OS是hosts文件</h3><ul><li><p>位置</p><ul><li>windows</li></ul><blockquote><p>C:\Windows\System32\drivers\etc\hosts</p></blockquote><ul><li>Linux</li></ul><blockquote><p>/etc/hosts</p></blockquote></li><li><p>作用</p></li></ul><blockquote><ul><li>加快域名解析</li></ul><blockquote><p>对于要经常访问的网站，我们可以通过在hosts中配置域名和IP的映射关系，提高域名解析速度。由于有了映射关系，当我们输入域名计算机就能很快解析出IP，而不用请求网络上的DNS服务器。</p><blockquote><p>注意：IP与域名中间要有空格，IP地址一定要输入正确，不然就访问不了该网站了，可以通过运行CMD，输入“ping 网站域名” 来获得网站的IP地址！）这样访问速度就会快一点啦，因为他不需要经过DNS域名服务器进行IP地址的解析！</p></blockquote></blockquote><ul><li>方便局域网用户</li></ul><blockquote><p>在很多单位的局域网中，会有服务器提供给用户使用。但由于局域网中一般很少架设DNS服务器，访问这些服务器时，要输入难记的IP地址，这对不少人来说相当麻烦，现在可以分别给这些服务器取个容易记住的名字，然后在hosts中建立IP映射，这样以后访问的时候，只要输入这个服务器的名字就行了。</p></blockquote><ul><li>屏蔽网站</li></ul><blockquote><p>现在有很多网站不经过用户同意就将各种各样的插件安装到你的计算机中，其中不乏有病毒木马。对于这些网站我们可以利用Hosts把该网站的域名映射到一个错误的IP或本地计算机的IP，这样就不用访问了。在Windows系统中，约定 127.0.0.1为本地计算机的IP地址, 0.0.0.0是错误的IP地址。</p><p>这个网站影响我孩子的健康成长，我要屏蔽他，这样，计算机解析域名www.sex.com 时，就解析到本机IP或错误的IP，达到了屏蔽不健康网站的目的。</p><p>现在某些病毒，恶意程序会修改我们的host文件，导致我们无法访问某些网站，当发现某些网站不能访问时，我们可以进入hosts文件进行观察，如果是因为hosts文件造成的网站无法访问，删除病毒添加的语句，就可以对网站进行正常访问。</p></blockquote></blockquote><ul><li><p>执行<figure class="highlight plain"><figcaption><span>/etc/hosts```centos6.7中的hosts文件内容</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&gt; 127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4</span><br><span class="line">&gt; ::1         localhost localhost.localdomain localhost6 localhost6.localdomain6</span><br><span class="line">&gt; 192.168.10.150 node01 zk01 kafka01 storm01</span><br><span class="line">&gt; 192.168.10.151 node02 zk02 kafka02 storm02</span><br><span class="line">&gt; 192.168.10.152 node03 zk03 kafka03 storm03</span><br><span class="line"></span><br><span class="line">### CentOS 6 时间，时区，设置修改及时间同步</span><br><span class="line"></span><br><span class="line">- 时区</span><br><span class="line"></span><br><span class="line">  - 显示时区</span><br><span class="line"></span><br><span class="line">    - date --help 获取帮助</span><br><span class="line"></span><br><span class="line">    - date -R</span><br><span class="line"></span><br><span class="line">    - date +%z</span><br><span class="line"></span><br><span class="line">      ```shell</span><br><span class="line">      [root@node01 ~]#  date -R; date +%z</span><br><span class="line">      Mon, 29 Jan 2018 06:20:28 +0800</span><br><span class="line">      +0800</span><br></pre></td></tr></table></figure></p><pre><code>&gt; 主要就是后面的+0800，东八区</code></pre><ul><li><p>修改时区</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime</span><br></pre></td></tr></table></figure><blockquote><p>时区的信息存在/usr/share/zoneinfo/下面，本机的时区信息存在/etc/localtime，利用tab键技巧，可以任意修改时区</p><p>tzselect，互动式命令</p></blockquote></li></ul></li><li><p>时间</p><ul><li><p>概念：Linux时间有两个</p></li><li><p>系统时间：也叫软件时间(sys)， 1970年1月1日到当前时间的秒数</p></li><li><p>BOIS时间：也叫硬件时间(hc)</p></li><li><p>显示时间</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@node01 ~]# date;hwclock -r</span><br><span class="line">Mon Jan 29 06:37:19 CST 2018</span><br><span class="line">Mon 29 Jan 2018 06:37:23 AM CST  -0.954874 seconds</span><br></pre></td></tr></table></figure></li><li><p>设置时间</p><ul><li><p>方式1：无网络环境</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@node01 ~]# date -s 20180201</span><br><span class="line">Thu Feb  1 00:00:00 CST 2018</span><br><span class="line">[root@node01 ~]# date -s 16:30:00</span><br><span class="line">Thu Feb  1 16:30:00 CST 2018</span><br></pre></td></tr></table></figure><blockquote><p>没有网络的情况下可以用这个</p></blockquote></li><li><p>方式2：可以连接外网(ntpdate)</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ntpdate time.windows.com &amp;&amp; hwclock -w</span><br></pre></td></tr></table></figure><blockquote><p>连网更新时间，如果成功，将系统时间，写入BOIS</p><p>hwclock -w 或 hwclock –systohc</p><p>可以做到crontab里</p></blockquote></li><li><p>方式3：启动ntpd服务，开启后2就不能用了</p><blockquote><p>先用ntpdate更新一下，确保时间不至于差别太大</p></blockquote><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">rpm -qa | grep ntp <span class="comment">#查询一下可安装了</span></span><br><span class="line">chkconfig --list | grep ntp <span class="comment">#看下服务情况</span></span><br><span class="line">chkconifg ntpd on</span><br><span class="line">service ntpd start 或/etc/init.d/ntpd start</span><br></pre></td></tr></table></figure><blockquote><p>必要的话，设置一下/etc/ntp.conf，再把服务reload一下。</p><p>ntp的知识参考一下鸟哥的服务器篇</p></blockquote></li></ul></li></ul></li></ul><h3 id="软件相关"><a href="#软件相关" class="headerlink" title="软件相关"></a>软件相关</h3><ul><li><p>安装规划</p><ul><li><p>软件版本号</p></li><li><p>依赖其他环境或包</p></li><li><p>软件存放路径</p></li><li><p>解压安装路径</p></li><li><p>软件日志文件存放路径</p></li><li><p>软件临时数据文件存放路径</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /export/server# 存放软件安装或解压文件</span><br><span class="line">mkdir -p /export/software# 存放软件包</span><br><span class="line">mkdir -p /export/data# 存放临时数据文件</span><br><span class="line">mkdir -p /export/log# 存放日志文件</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /export/server &amp;&amp; mkdir -p /export/software &amp;&amp; mkdir -p /export/data &amp;&amp; mkdir -p /export/log</span><br></pre></td></tr></table></figure><p>​</p></li></ul></li></ul><ul><li><p>安装流程</p><ul><li><p>将指定软件包(xxx.tar.gz)下载或者上传到A节点服务器</p></li><li><p>解压到指定路径</p></li><li><p>重命名或创建软链接</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mv zookeeperxxx zk</span><br><span class="line">mv jdk1.8xxx jdk</span><br><span class="line">ln -s zookeeperxxx zk</span><br><span class="line">ln -s jdk1.8xxx jdk</span><br></pre></td></tr></table></figure></li><li><p>创建日志文件、临时数据文件等文件的存放路径</p></li><li><p>修改配置文件</p></li><li><p>配置环境变量</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/profile</span><br></pre></td></tr></table></figure><blockquote><p>按Go跳到文件最后一行</p></blockquote></li><li><p>修改host映射</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/hosts</span><br></pre></td></tr></table></figure></li><li><p>分发到集群的其他相关节点</p><ul><li>软件包解压安装全路径</li><li>临时数据文件创建路径<ul><li>有的需要自己创建，有的自动创建</li><li>有的分发后无需修改，有的分发后需要修改</li></ul></li><li>如果节点环境变量文件相同可以分发，否则最好自己手动配置环境变量</li></ul></li><li><p>更新环境变量</p><ul><li>可以重启或者执行如下命令</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure><blockquote><p>可以使用XShell或SecureCRT下面的回话窗口同时在多节点执行命令</p></blockquote></li><li><p>自定义启动脚本/一键启动脚本/便捷个性化脚本</p></li></ul></li><li><p>JDK8安装</p><ul><li>查看是否安装</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rpm -qa | grep java</span><br></pre></td></tr></table></figure><ul><li>卸载自带openJDK</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">rpm -e --nodeps java-1.7.0-openjdk-1.7.0.45-2.4.3.3.el6.x86_64</span><br><span class="line"></span><br><span class="line"> rpm -e --nodeps java-1.6.0-openjdk-1.6.0.0-1.66.1.13.0.el6.x86_64</span><br><span class="line"></span><br><span class="line"> rpm -e --nodeps tzdata-java-2013g-1.el6.noarch</span><br></pre></td></tr></table></figure><ul><li>解压并配置环境变量</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 解压</span></span><br><span class="line">tar -zxvf /export/softwares/jdk-8u144-linux-x64.tar.gz -C /export/server</span><br><span class="line">ln -s jdk1.8.0_144 jdk8</span><br><span class="line"><span class="meta">#</span><span class="bash"> 打开环境变量配置文件</span></span><br><span class="line"> vim /etc/profile</span><br><span class="line"> or</span><br><span class="line"> vi /etc/profile</span><br><span class="line"> or</span><br><span class="line"> gedit /etc/profile</span><br></pre></td></tr></table></figure><ul><li>设置jdk环境变量</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">JAVA_HOME=/export/server/jdk1.8.0_144</span><br><span class="line">JRE_HOME=/export/server/jdk1.8.0_144/jre</span><br><span class="line">CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/lib</span><br><span class="line">PATH=$JAVA_HOME/bin:$PATH</span><br><span class="line">export PATH JAVA_HOME CLASSPATH</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">使jdk环境变量立即生效</span></span><br><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure><ul><li>测试jdk安装成功且环境变量配置成功</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">java -version</span><br><span class="line">java version "1.8.0_144"</span><br><span class="line">Java(TM) SE Runtime Environment (build 1.8.0_144-b01)</span><br><span class="line">Java HotSpot(TM) 64-Bit Server VM (build 25.144-b01, mixed mode)</span><br><span class="line">javac -version</span><br><span class="line">javac 1.8.0_144</span><br></pre></td></tr></table></figure><p>分发到其他节点</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scp -r /export/server/jdk8 vm02:/export/server/</span><br><span class="line">scp /etc/profile vm02:/etc/</span><br></pre></td></tr></table></figure><p>​</p></li><li><p>mysql安装</p></li><li><p>redis安装</p></li><li><p>nginx安装</p></li><li><p>tomcat安装</p></li><li><p>solr安装</p></li><li><p>ELK安装</p></li><li><p>zookeeper安装</p><p>zookeeper安装依赖于java JDK，需要先安装JDK，再解压安装zk，可以使用JDK的jps查看zk进程判断是否运行</p><ul><li>下载或上传安装包</li><li>解压</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf zookeeper-3.4.5.tar.gz</span><br><span class="line"><span class="meta">#</span><span class="bash">或者使用如下命令解压到指定路径</span></span><br><span class="line"><span class="meta">#</span><span class="bash">tar -zxvf zookeeper-3.4.5.tar.gz -C /<span class="built_in">export</span>/server</span></span><br><span class="line">mv zookeeper-3.4.5 zk</span><br></pre></td></tr></table></figure><ul><li>修改zookeeper配置文件</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /export/server/zk/conf</span><br><span class="line">cp zoo_sample.cfg zoo.cfg</span><br><span class="line">vim zoo.cfg</span><br></pre></td></tr></table></figure><ul><li>zoo.cfg文件添加如下内容</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 客户端连接端口，默认2181,可以省略</span></span><br><span class="line">clientPort = 2181</span><br><span class="line"><span class="meta">#</span><span class="bash"> 存储内存数据库快照的位置和放置事务日志的位置</span></span><br><span class="line">dataDir=/export/data/zookeeper</span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># (2888心跳端口、3888选举端口)</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> server.x = [主机名]：nnnnn [：nnnnn]</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 组成ZooKeeper集合的服务器</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> zk01,zk02,zk03是配置在/etc/hosts中，可以不单独配置，直接使用原来的node01,node02,node03</span></span><br><span class="line">server.1=zk01:2888:3888</span><br><span class="line">server.2=zk02:2888:3888</span><br><span class="line">server.3=zk03:2888:3888</span><br></pre></td></tr></table></figure><ul><li>创建数据库快照文件存储路径</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /export/data/zookeeper</span><br></pre></td></tr></table></figure><ul><li>创建zk集群节点服务器编号文件myid</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo 1 &gt; /export/server</span><br></pre></td></tr></table></figure><p>​</p><ul><li>将zk加入环境变量</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/profile</span><br><span class="line"><span class="meta">#</span><span class="bash">按键盘Go到文件末尾新空行</span></span><br><span class="line"><span class="meta">#</span><span class="bash">添加如下内容</span></span><br><span class="line"><span class="meta">#</span><span class="bash">设置zookeeper环境变量</span></span><br><span class="line">export ZK_HOME=/export/server/zookeeper-3.4.5</span><br><span class="line"><span class="meta">#</span><span class="bash">或者使用</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">export</span> ZK_HOME=/<span class="built_in">export</span>/server/zk</span></span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin:$ZK_HOME/bin</span><br></pre></td></tr></table></figure><ul><li><p>执行<figure class="highlight plain"><figcaption><span>/etc/profile```使得环境变量立即生效</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">- 分发到zk集群的其他节点</span><br><span class="line"></span><br><span class="line">```shell</span><br><span class="line">#在zoo.cfg文件dataDir=/export/data/zookeeper指定位置创建myid</span><br><span class="line">#可以通过scp分发后修改myid文件中的数值或者到对应节点创建myid文件并写入对应编号，注意是依次编号不能重复</span><br><span class="line"></span><br><span class="line">###zk02节点的zk包和myid文件</span><br><span class="line">scp -r /export/server/zk node02:/export/server/</span><br><span class="line">scp -r /export/data/zookeeper/myid node02:/export/data/zookeeper</span><br><span class="line">echo 2 &gt; /export/data/zookeeper/myid</span><br><span class="line">###zk03节点zk包和myid文件</span><br><span class="line">scp -r /export/server/zk node03:/export/server/</span><br><span class="line">mkdir -p /export/data/zookeeper</span><br><span class="line">echo 3 &gt; /export/data/zookeeper/myid</span><br><span class="line">#配置或分发zk01环境变量zk02和zk03节点</span><br><span class="line">scp /etc/profile node02:/etc/profile</span><br><span class="line">scp /etc/profile node03:/etc/profile</span><br></pre></td></tr></table></figure></p></li><li><p>测试</p></li><li>输入zk按两次tab键看zk相关命令脚本是否出现</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@node02 ~]# zk</span><br><span class="line">zkCleanup.sh  zkCli.cmd     zkCli.sh      zkEnv.cmd     zkEnv.sh      zkServer.cmd  zkServer.sh</span><br></pre></td></tr></table></figure><ul><li>启动zk集群</li><li><p>分别在zk01、zk02和zk03上执行<figure class="highlight plain"><figcaption><span>start```</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">```shell</span><br><span class="line">[root@node01 ~]# zkServer.sh start</span><br><span class="line">JMX enabled by default</span><br><span class="line">Using config: /export/servers/zookeeper-3.4.5/bin/../conf/zoo.cfg</span><br><span class="line">Starting zookeeper ... STARTED</span><br></pre></td></tr></table></figure></p></li><li><p>执行<figure class="highlight plain"><figcaption><span>server的进程**QuorumPeerMain**</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">```shell</span><br><span class="line">[root@node01 ~]# jps</span><br><span class="line">1894 QuorumPeerMain</span><br><span class="line">1919 Jps</span><br></pre></td></tr></table></figure></p></li><li><p>执行zkServer.sh status查看每个zk的运行状态及角色</p></li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@node01 ~]# zkServer.sh status</span><br><span class="line">JMX enabled by default</span><br><span class="line">Using config: /export/servers/zookeeper-3.4.5/bin/../conf/zoo.cfg</span><br><span class="line">Mode: follower</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@node02 ~]# zkServer.sh status</span><br><span class="line">JMX enabled by default</span><br><span class="line">Using config: /export/servers/zookeeper-3.4.5/bin/../conf/zoo.cfg</span><br><span class="line">Mode: leader</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@node03 ~]# zkServer.sh status</span><br><span class="line">JMX enabled by default</span><br><span class="line">Using config: /export/servers/zookeeper-3.4.5/bin/../conf/zoo.cfg</span><br><span class="line">Mode: follower</span><br></pre></td></tr></table></figure><p>执行<figure class="highlight plain"><figcaption><span>stop```停止zk服务</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">  &gt; 3台zk集群的容错能力是1台（4台也是1）</span><br><span class="line">  &gt;</span><br><span class="line">  &gt; 其中一台挂掉后，另外两台可以继续正常工作</span><br><span class="line">  &gt;</span><br><span class="line">  &gt; 如果leader挂掉，重新投票选举leader</span><br><span class="line">  &gt;</span><br><span class="line">  &gt; 如果follower挂掉，leader不会改变</span><br><span class="line">  &gt;</span><br><span class="line">  &gt; 如果挂掉两台，剩下的一台zk节点无法正常工作</span><br><span class="line">  &gt;</span><br><span class="line">  &gt; 5台的zk集群的容错能力是2台，即最多允许两台zk节点服务器挂掉还能正常 工作。（6台也是2）</span><br><span class="line">  &gt;</span><br><span class="line">  &gt; 小结：zk集群一般是基数台（2n+1）,必须保证半数以上机器存活才能正常工作</span><br><span class="line"></span><br><span class="line">  ​</span><br><span class="line"></span><br><span class="line">  &gt; 集群配置参数节选</span><br><span class="line">  &gt;</span><br><span class="line">  &gt; - server.x=[hostname]:nnnnn[:nnnnn], etc</span><br><span class="line">  &gt;</span><br><span class="line">  &gt; &gt; (No Java system property)</span><br><span class="line">  &gt; &gt;</span><br><span class="line">  &gt; &gt; servers making up the ZooKeeper ensemble. When the server starts up, it determines which server it is by looking for the file myid in the data directory. That file contains the server number, in ASCII, and it should match **x** in **server.x** in the left hand side of this setting.</span><br><span class="line">  &gt; &gt;</span><br><span class="line">  &gt; &gt; The list of servers that make up ZooKeeper servers that is used by the clients must match the list of ZooKeeper servers that each ZooKeeper server has.</span><br><span class="line">  &gt; &gt;</span><br><span class="line">  &gt; &gt; There are two port numbers **nnnnn**. The first followers use to connect to the leader, and the second is for leader election. The leader election port is only necessary if electionAlg is 1, 2, or 3 (default). If electionAlg is 0, then the second port is not necessary. If you want to test multiple servers on a single machine, then different ports can be used for each server.</span><br><span class="line">  &gt;</span><br><span class="line">  &gt; ​</span><br><span class="line">  &gt;</span><br><span class="line">  &gt; - electionAlg</span><br><span class="line">  &gt;</span><br><span class="line">  &gt; &gt; (No Java system property)</span><br><span class="line">  &gt; &gt;</span><br><span class="line">  &gt; &gt; Election implementation to use. </span><br><span class="line">  &gt; &gt;</span><br><span class="line">  &gt; &gt; - A value of &quot;0&quot; corresponds to the original UDP-based version, </span><br><span class="line">  &gt; &gt; - &quot;1&quot; corresponds to the non-authenticated UDP-based version of fast leader election,</span><br><span class="line">  &gt; &gt; - &quot;2&quot; corresponds to the authenticated UDP-based version of fast leader election, </span><br><span class="line">  &gt; &gt; - and &quot;3&quot; corresponds to TCP-based version of fast leader election. </span><br><span class="line">  &gt; &gt;</span><br><span class="line">  &gt; &gt; Currently, algorithm 3 is the default</span><br><span class="line">  &gt; &gt;</span><br><span class="line">  &gt; &gt; &gt; Note</span><br><span class="line">  &gt; &gt; &gt;</span><br><span class="line">  &gt; &gt; &gt; The implementations of leader election 0, 1, and 2 are now **deprecated **. We have the intention of removing them in the next release, at which point only the FastLeaderElection will be available.</span><br><span class="line"></span><br><span class="line">  [集群配置选项](http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_configuration)</span><br><span class="line"></span><br><span class="line">  [electionAlg=0集群无法恢复且日志无限打印的问题](https://issues.apache.org/jira/browse/ZOOKEEPER-2400)</span><br><span class="line"></span><br><span class="line">  - zookeeper的高性能参数配置</span><br><span class="line"></span><br><span class="line">  &gt; 为了在更新时获得较低的延迟，有一个专用的事务日志目录很重要。默认情况下，事务日志与数据快照和*myid*文件放在同一个目录中。dataLogDir参数指示用于事务日志的不同目录。</span><br><span class="line"></span><br><span class="line">  - 为什么需要配置两个端口server.1=zk01:2888:3888？</span><br><span class="line"></span><br><span class="line">  &gt;  follower连接到leader并与leader进行通信的TCP连接端口</span><br><span class="line">  &gt;</span><br><span class="line">  &gt; 默认的leader选举也是TCP协议，所以另一个端口作为leader选举端口。**即一个TCP协议通信端口，一个TCP协议leader选举端口（默认electionAlg=3，FastLeaderElection算法）**</span><br><span class="line"></span><br><span class="line">  zookeeper的安装运行</span><br><span class="line"></span><br><span class="line">- flume安装</span><br><span class="line"></span><br><span class="line">- hadoop安装</span><br><span class="line"></span><br><span class="line">- hive安装</span><br><span class="line"></span><br><span class="line">- azkaban安装</span><br><span class="line"></span><br><span class="line">- kafka安装</span><br><span class="line"></span><br><span class="line">- storm安装</span><br><span class="line"></span><br><span class="line">- sqoop安装</span><br><span class="line"></span><br><span class="line">- spark安装</span><br><span class="line"></span><br><span class="line">### 安装方式</span><br><span class="line"></span><br><span class="line">- 离线安装</span><br><span class="line">  - xxx.tar.gz的压缩包需要解压安装，有的需要解压+编译+安装</span><br><span class="line">  - rpm文件安装</span><br><span class="line"></span><br><span class="line">- 在线安装</span><br><span class="line"></span><br><span class="line">  - 对于某些包或软件可以选择在线安装</span><br><span class="line"></span><br><span class="line">  - 简单方便</span><br><span class="line"></span><br><span class="line">    ```shell</span><br><span class="line">    yum install nc</span><br><span class="line">    yum install lrzsz</span><br></pre></td></tr></table></figure></p><ul><li>如果yum源中的软件版本过低，不满足需求，需要离线安装</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;#分布式集群&lt;/p&gt;
&lt;p&gt;###前置事项&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;规划集群&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Linux存储、系统及版本选择&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ubuntu16.x&lt;/li&gt;
&lt;li&gt;Centos6.x&lt;/li&gt;
&lt;li&gt;主节点一般选大内存
      
    
    </summary>
    
      <category term="bigdata" scheme="http://www.bluedreams.top/categories/bigdata/"/>
    
    
      <category term="bigdata" scheme="http://www.bluedreams.top/tags/bigdata/"/>
    
  </entry>
  
  <entry>
    <title>tags</title>
    <link href="http://www.bluedreams.top/2018/02/06/tags/"/>
    <id>http://www.bluedreams.top/2018/02/06/tags/</id>
    <published>2018-02-06T14:25:20.000Z</published>
    <updated>2018-02-06T14:28:51.610Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://www.bluedreams.top/2018/02/06/README/"/>
    <id>http://www.bluedreams.top/2018/02/06/README/</id>
    <published>2018-02-06T13:27:06.639Z</published>
    <updated>2018-02-06T13:27:06.639Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>关于_config.yml文件</title>
    <link href="http://www.bluedreams.top/2018/01/28/hexo_config.yml/"/>
    <id>http://www.bluedreams.top/2018/01/28/hexo_config.yml/</id>
    <published>2018-01-28T09:05:00.007Z</published>
    <updated>2018-01-28T09:29:18.989Z</updated>
    
    <content type="html"><![CDATA[<h1 id="主要参数介绍"><a href="#主要参数介绍" class="headerlink" title="主要参数介绍"></a>主要参数介绍</h1><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Hexo Configuration</span></span><br><span class="line"><span class="comment">## Docs: https://hexo.io/docs/configuration.html</span></span><br><span class="line"><span class="comment">## Source: https://github.com/hexojs/hexo/</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Site</span></span><br><span class="line"><span class="attr">title:</span> <span class="string">Hexo</span> <span class="comment">#网站标题</span></span><br><span class="line"><span class="attr">subtitle:</span>   <span class="comment">#网站副标题</span></span><br><span class="line"><span class="attr">description:</span>  <span class="comment">#网站描述</span></span><br><span class="line"><span class="attr">author:</span> <span class="string">John</span> <span class="string">Doe</span>  <span class="comment">#作者</span></span><br><span class="line"><span class="attr">language:</span>    <span class="comment">#语言</span></span><br><span class="line"><span class="attr">timezone:</span>    <span class="comment">#网站时区。Hexo 默认使用您电脑的时区。时区列表。比如说：America/New_York, Japan, 和 UTC 。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># URL</span></span><br><span class="line"><span class="comment">## If your site is put in a subdirectory, set url as 'http://yoursite.com/child' and root as '/child/'</span></span><br><span class="line"><span class="attr">url:</span> <span class="attr">http://yoursite.com</span>   <span class="comment">#你的站点Url</span></span><br><span class="line"><span class="attr">root:</span> <span class="string">/</span>                       <span class="comment">#站点的根目录</span></span><br><span class="line"><span class="attr">permalink:</span> <span class="string">:year/:month/:day/:title/</span>   <span class="comment">#文章的 永久链接 格式   </span></span><br><span class="line"><span class="attr">permalink_defaults:</span>    <span class="comment">#永久链接中各部分的默认值</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Directory   </span></span><br><span class="line"><span class="attr">source_dir:</span> <span class="string">source</span>   <span class="comment">#资源文件夹，这个文件夹用来存放内容</span></span><br><span class="line"><span class="attr">public_dir:</span> <span class="string">public</span>     <span class="comment">#公共文件夹，这个文件夹用于存放生成的站点文件。</span></span><br><span class="line"><span class="attr">tag_dir:</span> <span class="string">tags</span>         <span class="comment"># 标签文件夹     </span></span><br><span class="line"><span class="attr">archive_dir:</span> <span class="string">archives</span>    <span class="comment">#归档文件夹</span></span><br><span class="line"><span class="attr">category_dir:</span> <span class="string">categories</span>      <span class="comment">#分类文件夹</span></span><br><span class="line"><span class="attr">code_dir:</span> <span class="string">downloads/code</span>     <span class="comment">#Include code 文件夹</span></span><br><span class="line"><span class="attr">i18n_dir:</span> <span class="string">:lang</span>                <span class="comment">#国际化（i18n）文件夹</span></span><br><span class="line"><span class="attr">skip_render:</span>                <span class="comment">#跳过指定文件的渲染，您可使用 glob 表达式来匹配路径。    </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Writing</span></span><br><span class="line"><span class="attr">new_post_name:</span> <span class="string">:title.md</span> <span class="comment"># 新文章的文件名称</span></span><br><span class="line"><span class="attr">default_layout:</span> <span class="string">post</span>     <span class="comment">#预设布局</span></span><br><span class="line"><span class="attr">titlecase:</span> <span class="literal">false</span> <span class="comment"># 把标题转换为 title case</span></span><br><span class="line"><span class="attr">external_link:</span> <span class="literal">true</span> <span class="comment"># 在新标签中打开链接</span></span><br><span class="line"><span class="attr">filename_case:</span> <span class="number">0</span>     <span class="comment">#把文件名称转换为 (1) 小写或 (2) 大写</span></span><br><span class="line"><span class="attr">render_drafts:</span> <span class="literal">false</span>  <span class="comment">#是否显示草稿</span></span><br><span class="line"><span class="attr">post_asset_folder:</span> <span class="literal">false</span>  <span class="comment">#是否启动 Asset 文件夹</span></span><br><span class="line"><span class="attr">relative_link:</span> <span class="literal">false</span>      <span class="comment">#把链接改为与根目录的相对位址    </span></span><br><span class="line"><span class="attr">future:</span> <span class="literal">true</span>                <span class="comment">#显示未来的文章</span></span><br><span class="line"><span class="attr">highlight:</span>                    <span class="comment">#内容中代码块的设置    </span></span><br><span class="line"><span class="attr">  enable:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">  line_number:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">  auto_detect:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">  tab_replace:</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Category &amp; Tag</span></span><br><span class="line"><span class="attr">default_category:</span> <span class="string">uncategorized</span></span><br><span class="line"><span class="attr">category_map:</span>          <span class="comment">#分类别名</span></span><br><span class="line"><span class="attr">tag_map:</span>            <span class="comment">#标签别名</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Date / Time format</span></span><br><span class="line"><span class="comment">## Hexo uses Moment.js to parse and display date</span></span><br><span class="line"><span class="comment">## You can customize the date format as defined in</span></span><br><span class="line"><span class="comment">## http://momentjs.com/docs/#/displaying/format/</span></span><br><span class="line"><span class="attr">date_format:</span> <span class="string">YYYY-MM-DD</span>         <span class="comment">#日期格式</span></span><br><span class="line"><span class="attr">time_format:</span> <span class="attr">HH:mm:ss</span>        <span class="comment">#时间格式    </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Pagination</span></span><br><span class="line"><span class="comment">## Set per_page to 0 to disable pagination</span></span><br><span class="line"><span class="attr">per_page:</span> <span class="number">10</span>    <span class="comment">#分页数量</span></span><br><span class="line"><span class="attr">pagination_dir:</span> <span class="string">page</span>  </span><br><span class="line"></span><br><span class="line"><span class="comment"># Extensions</span></span><br><span class="line"><span class="comment">## Plugins: https://hexo.io/plugins/</span></span><br><span class="line"><span class="comment">## Themes: https://hexo.io/themes/</span></span><br><span class="line"><span class="attr">theme:</span> <span class="string">landscape</span>   <span class="comment">#主题名称</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Deployment</span></span><br><span class="line"><span class="comment">## Docs: https://hexo.io/docs/deployment.html</span></span><br><span class="line"><span class="comment">#  部署部分的设置</span></span><br><span class="line"><span class="attr">deploy:</span>     </span><br><span class="line"><span class="attr">  type:</span>  <span class="comment">#类型，常用的git</span></span><br></pre></td></tr></table></figure><p><a href="https://my.oschina.net/u/1861837/blog/526142" target="_blank" rel="noopener">_config.yml 的YAML语法格式</a></p><p><a href="https://hexo.io/zh-cn/docs/configuration.html" target="_blank" rel="noopener">_config.yml配置官方文档</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;主要参数介绍&quot;&gt;&lt;a href=&quot;#主要参数介绍&quot; class=&quot;headerlink&quot; title=&quot;主要参数介绍&quot;&gt;&lt;/a&gt;主要参数介绍&lt;/h1&gt;&lt;figure class=&quot;highlight yaml&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutt
      
    
    </summary>
    
      <category term="hexo" scheme="http://www.bluedreams.top/categories/hexo/"/>
    
    
      <category term="hexo" scheme="http://www.bluedreams.top/tags/hexo/"/>
    
  </entry>
  
  <entry>
    <title>hexo常用命令</title>
    <link href="http://www.bluedreams.top/2018/01/28/hexo/"/>
    <id>http://www.bluedreams.top/2018/01/28/hexo/</id>
    <published>2018-01-28T08:37:40.427Z</published>
    <updated>2018-02-07T01:37:45.773Z</updated>
    
    <content type="html"><![CDATA[<h1 id="常见命令"><a href="#常见命令" class="headerlink" title="常见命令"></a>常见命令</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">hexo new <span class="string">"postName"</span> <span class="comment">#新建文章</span></span><br><span class="line">hexo new page <span class="string">"pageName"</span> <span class="comment">#新建页面</span></span><br><span class="line">hexo generate <span class="comment">#生成静态页面至public目录</span></span><br><span class="line">hexo server <span class="comment">#开启预览访问端口（默认端口4000，'ctrl + c'关闭server）</span></span><br><span class="line">hexo deploy <span class="comment">#部署到GitHub</span></span><br><span class="line">hexo <span class="built_in">help</span>  <span class="comment"># 查看帮助</span></span><br><span class="line">hexo version  <span class="comment">#查看Hexo的版本</span></span><br><span class="line"><span class="comment">#生成博文是执行如下命令</span></span><br><span class="line"><span class="comment">#根据 gulpfile.js 中的配置</span></span><br><span class="line"><span class="comment">#对 public 目录中的静态资源文件进行压缩</span></span><br><span class="line">hexo g &amp;&amp; gulp</span><br></pre></td></tr></table></figure><h1 id="缩写"><a href="#缩写" class="headerlink" title="缩写"></a>缩写</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hexo n == hexo new</span><br><span class="line">hexo g == hexo generate</span><br><span class="line">hexo s == hexo server</span><br><span class="line">hexo d == hexo deploy</span><br></pre></td></tr></table></figure><h1 id="组合命令"><a href="#组合命令" class="headerlink" title="组合命令"></a>组合命令</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hexo s -g <span class="comment">#生成并本地预览</span></span><br><span class="line">hexo d -g <span class="comment">#生成并上传</span></span><br></pre></td></tr></table></figure><p>修改后本地测试已经变化但是提交到github上无变化，请依次执行下面命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hexo clean </span><br><span class="line">hexo g  </span><br><span class="line">hexo d</span><br></pre></td></tr></table></figure><p><a href="http://ijiaober.github.io/2014/08/07/hexo" target="_blank" rel="noopener">参考Hexo的配置使用</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;常见命令&quot;&gt;&lt;a href=&quot;#常见命令&quot; class=&quot;headerlink&quot; title=&quot;常见命令&quot;&gt;&lt;/a&gt;常见命令&lt;/h1&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre
      
    
    </summary>
    
      <category term="hexo" scheme="http://www.bluedreams.top/categories/hexo/"/>
    
    
      <category term="hexo" scheme="http://www.bluedreams.top/tags/hexo/"/>
    
  </entry>
  
  <entry>
    <title>zookeeper</title>
    <link href="http://www.bluedreams.top/2018/01/28/zookeeper/"/>
    <id>http://www.bluedreams.top/2018/01/28/zookeeper/</id>
    <published>2018-01-28T08:32:42.247Z</published>
    <updated>2018-02-06T14:57:06.176Z</updated>
    
    <content type="html"><![CDATA[<h1 id="zookeeper"><a href="#zookeeper" class="headerlink" title="zookeeper"></a>zookeeper</h1><h4 id="是什么"><a href="#是什么" class="headerlink" title="是什么"></a>是什么</h4><ul><li>一个分布式协调服务的开源框架（抽象/高大上）</li><li>一个分布式小文件存储系统（形象具体）</li></ul><ul><li>数据存储方式<ul><li>文件系统目录树<ul><li>便于对树节点进行有效管理</li><li>便于监控维护存储的数据的状态变化，从而达到基于数据的集群管理<ul><li>统一命名服务</li><li>分布式配置管理</li><li>分布式消息队列</li><li>分布式锁</li><li>分布式协调</li></ul></li></ul></li></ul></li></ul><h4 id="主要作用"><a href="#主要作用" class="headerlink" title="主要作用"></a>主要作用</h4><ul><li>解决分布式集群中应用系统一致性问题（避免同时操作同一数据造成脏读等）</li></ul><h4 id="角色与职能"><a href="#角色与职能" class="headerlink" title="角色与职能"></a>角色与职能</h4><ul><li>leader<ul><li>集群工作的核心</li><li>事务请求（写操作）的唯一调度者和处理者，保证集群事务处理的顺序性</li><li>集群内部各个服务器的调度者<ul><li>写操作<ul><li>create</li><li>setData</li><li>delete</li></ul></li></ul></li></ul></li><li>follower<ul><li>处理客户端非事务请求（读操作），事务请求转发给leader</li><li>参与集群leader的选举投票</li></ul></li><li>observer<ul><li>观察zookeeper集群的最新状态变化并同步</li><li>可以独立处理客户端的非事务请求，事务请求转发给leader</li><li>不参与任何形式的投票</li><li>通常在不影响集群事务处理能力的前提下提升的非事务处理能力</li><li>case:某电商项目读压力较大，可以在zk cluster中添加若干台server充当observer，用于水平扩展</li></ul></li></ul><h4 id="特性"><a href="#特性" class="headerlink" title="特性"></a>特性</h4><ul><li>全局数据一致性<ul><li>每个server保存一份相同的数据副本，client连接到哪个server，看到的数据都一致</li></ul></li><li>可靠性<ul><li>如果消息被其中一台服务器接收，那么将被所有服务器接收</li></ul></li><li>顺序性<ul><li>全局有序<ul><li>如果在一台服务器上，消息a在消息b前发布，那么在所有server上都是消息a在消息b前发布</li></ul></li><li>偏序<ul><li>如果一消息b在消息a之后被同一发送者发布，那么a必将排在b的前面</li></ul></li></ul></li><li>数据更新的原子性<ul><li>一次数据更新要么全部成功（半数以上节点成功），要么全部失败，没有中间状态</li></ul></li><li>实时性<ul><li>zookeeper保证客户端在一定的时间间隔内获得服务器的更新信息或者服务器的失效信息</li></ul></li></ul><h4 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h4><p>​    以下是设置服务器的步骤，这些服务器将成为整体的一部分。这些步骤应该在集合中的每个主机上执行：</p><ol><li><p>安装Java JDK。您可以在您的系统上使用本地打包系统，或从以下位置下载JDK：</p><p><a href="http://java.sun.com/javase/downloads/index.jsp" target="_blank" rel="noopener">http://java.sun.com/javase/downloads/index.jsp</a></p></li><li><p>设置Java堆大小。这是非常重要的，以避免交换，这将严重降低ZooKeeper性能。要确定正确的值，请使用负载测试，并确保您远低于可能导致交换的使用限制。保守 - 对于4GB机器，最大堆大小为3GB。</p></li><li><p>安装ZooKeeper服务器包。它可以从以下网址下载：</p><p><a href="http://zookeeper.apache.org/releases.html" target="_blank" rel="noopener">http://zookeeper.apache.org/releases.html</a></p></li><li><p>创建一个配置文件。这个文件可以被称为任何东西。使用以下设置作为起点：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tickTime = 2000 </span><br><span class="line">dataDir = / var / lib / zookeeper / </span><br><span class="line">clientPort = 2181 </span><br><span class="line">initLimit = 5 </span><br><span class="line">syncLimit = 2 </span><br><span class="line">server.1 = zoo1：2888：3888 </span><br><span class="line">server.2 = zoo2：2888：3888 </span><br><span class="line">server.3 = zoo3：2888：3888</span><br></pre></td></tr></table></figure><p>您可以在<a href="http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_configuration" target="_blank" rel="noopener">配置参数</a>部分找到这些配置和其他配置设置的含义。一个字虽然在这里几个：</p><p>每个属于ZooKeeper集合的机器都应该知道集合中的每一台机器。您可以使用<strong>server.id = host：port：port</strong>格式的一系列行完成此操作。参数<strong>主机</strong>和<strong>端口</strong>是直接的。通过创建一个名为myid的文件（每个服务器驻留在该服务器的数据目录中），将服务器ID分配给每台计算机 ，如配置文件参数<strong>dataDir</strong>所指定的。</p></li><li><p>myid文件由一行仅包含该机器ID的文本组成。所以服务器1的myid将包含文本“1”，而不是别的。id在集合中必须是唯一的，并且应该具有1到255之间的值。</p></li><li><p>如果你的配置文件设置好了，你可以启动一个ZooKeeper服务器：</p><p>$ java -cp zookeeper.jar：lib / slf4j-api-1.6.1.jar：lib / slf4j-log4j12-1.6.1.jar：lib / log4j-1.2.15.jar：conf \ org.apache.zookeeper。 server.quorum.QuorumPeerMain zoo.cfg</p><p>QuorumPeerMain启动一个ZooKeeper服务器， <a href="http://java.sun.com/javase/technologies/core/mntr-mgmt/javamanagement/" target="_blank" rel="noopener">JMX</a> 管理bean也被注册，允许通过一个JMX管理控制台进行管理。该<a href="http://zookeeper.apache.org/doc/current/zookeeperJMX.html" target="_blank" rel="noopener">ZooKeeper的JMX文件</a>包含有关使用JMX管理ZooKeeper的细节。</p><p>有关启动服务器实例的示例，请参阅发行版中包含的脚本<em>bin / zkServer.sh</em>。</p></li><li><p>通过连接到主机来测试您的部署：</p><p>在Java中，您可以运行以下命令来执行简单的操作：</p><p>$ bin / zkCli.sh -server 127.0.0.1:2181</p><p><a href="http://zookeeper.apache.org/doc/current/zookeeperAdmin.html" target="_blank" rel="noopener">Apache官方安装说明</a></p></li></ol><h4 id="安装方式"><a href="#安装方式" class="headerlink" title="安装方式"></a>安装方式</h4><ul><li>单机模式</li><li>伪集群模式</li><li>集群模式</li></ul><h4 id="运行模式"><a href="#运行模式" class="headerlink" title="运行模式"></a>运行模式</h4><ul><li>单机模式<ul><li>standalone mode</li><li>便于评估、开发和测试</li><li>主要用于开发测试环境</li></ul></li><li>集群模式<ul><li>replicated mode</li><li>用于生产环境</li></ul></li></ul><h4 id="一个zookeeper集群的机器数"><a href="#一个zookeeper集群的机器数" class="headerlink" title="一个zookeeper集群的机器数"></a>一个zookeeper集群的机器数</h4><ul><li>仲裁集（<em>quorum</em>）</li><li>有相同的配置文件副本</li><li>至少需要三台服务器，为了更好的容错能力和更大的可靠性可以配置五台</li><li>官方强烈推荐基数台</li><li>半数以上节点存活才能正常运行</li><li>3台服务器和4台服务器的的容灾能力是一样的，所以为了节省服务器资源，一般我们采用奇数个数，作为服务器部署个数</li></ul><blockquote><p>Note</p><p>As mentioned in the <a href="http://zookeeper.apache.org/doc/current/zookeeperStarted.html" target="_blank" rel="noopener">ZooKeeper Getting Started Guide</a> , a minimum of three servers are required for a fault tolerant clustered setup, and it is strongly recommended that you have an odd number of servers.</p><p>Usually three servers is more than enough for a production install, but for maximum reliability during maintenance, you may wish to install five servers. With three servers, if you perform maintenance on one of them, you are vulnerable to a failure on one of the other two servers during that maintenance. If you have five of them running, you can take one down for maintenance, and know that you’re still OK if one of the other four suddenly fails.</p><p>Your redundancy considerations should include all aspects of your environment. If you have three ZooKeeper servers, but their network cables are all plugged into the same network switch, then the failure of that switch will take down your entire ensemble.</p><p>容错集群设置至少需要三台服务器，强烈建议您有奇数个服务器</p><p>通常情况下，对于一个生产环境安装三台服务器足够了，但是在运维期间，为了最大的可靠性，你可能希望安装五台服务器。在三台服务器的集群中，如果你正在其中一台上进行维护操作，在维护期间，你很容易受到其他两台服务器中的一台出现故障的影响。如果有五台服务器正在运行，你可以维护其中一台并且知道如果其他四台中的一台宕机仍然可以正常工作。</p><p>你的冗余考虑应该包括所有环境方面的因素。如果你有三台zookeeper服务器，但他们的网络电缆都插进了相同的网络交换机，那么交换机故障会让你的整个集群宕掉。</p></blockquote><p><a href="http://zookeeper.apache.org/doc/current/zookeeperAdmin.html" target="_blank" rel="noopener">请重点参阅</a></p><h4 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h4><ul><li>主从vs主备<ul><li>主从：各个节点数据不一致</li><li>主备：各个服务器的数据一致，互为副本</li></ul></li><li>数据副本<ul><li>包括自己</li></ul></li></ul><h4 id="选举机制"><a href="#选举机制" class="headerlink" title="选举机制"></a>选举机制</h4><h4 id="相关协议"><a href="#相关协议" class="headerlink" title="相关协议"></a>相关协议</h4><p>ZAB（zookeeper automatic broadcast）</p><p><a href="http://www.cnblogs.com/peerslee/p/8144625.html" target="_blank" rel="noopener">ZAB 协议</a></p><h4 id="相关算法"><a href="#相关算法" class="headerlink" title="相关算法"></a>相关算法</h4><ul><li>Paxos</li><li>FastLeaderElection</li></ul><h4 id="ZooKeeper、Eureka对比"><a href="#ZooKeeper、Eureka对比" class="headerlink" title="ZooKeeper、Eureka对比"></a>ZooKeeper、Eureka对比</h4><blockquote><p> Eureka本身是Netflix开源的一款提供服务注册和发现的产品，并且提供了相应的Java封装。在它的实现中，节点之间相互平等，部分注册中心的节点挂掉也不会对集群造成影响，即使集群只剩一个节点存活，也可以正常提供发现服务。哪怕是所有的服务注册节点都挂了，Eureka Clients（客户端）上也会缓存服务调用的信息。这就保证了我们微服务之间的互相调用足够健壮。</p><p>Zookeeper主要为大型分布式计算提供开源的分布式配置服务、同步服务和命名注册。曾经是Hadoop项目中的一个子项目，用来控制集群中的数据，目前已升级为独立的顶级项目。很多场景下也用它作为Service发现服务解决方案。</p><p>在分布式系统中有个著名的CAP定理（C-数据一致性；A-服务可用性；P-服务对网络分区故障的容错性，这三个特性在任何分布式系统中不能同时满足，最多同时满足两个）；</p></blockquote><ul><li><p>ZooKeeper基于CP，不保证高可用，如果zookeeper正在选主，或者Zookeeper集群中半数以上机器不可用，那么将无法获得数据。Eureka基于AP，能保证高可用，即使所有机器都挂了，也能拿到本地缓存的数据。作为注册中心，其实配置是不经常变动的，只有发版和机器出故障时会变。对于不经常变动的配置来说，CP是不合适的，而AP在遇到问题时可以用牺牲一致性来保证可用性，既返回旧数据，缓存数据。</p><p>所以理论上Eureka是更适合作注册中心。而现实环境中大部分项目可能会使用ZooKeeper，那是因为集群不够大，并且也比基本不会遇到用做注册中心的机器一半以上都挂了的情况。所以实际上也没什么大问题。</p></li><li><p><a href="http://tech.lede.com/2017/03/15/rd/server/SpringCloud1/" target="_blank" rel="noopener">参考</a></p></li><li><p><a href="https://www.cnblogs.com/jieqing/p/8394001.html" target="_blank" rel="noopener">原文地址</a></p></li><li><p><a href="https://mp.weixin.qq.com/s/8Vul5mBo2ReQYQL3nX81XA" target="_blank" rel="noopener">java葵花宝典</a></p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;zookeeper&quot;&gt;&lt;a href=&quot;#zookeeper&quot; class=&quot;headerlink&quot; title=&quot;zookeeper&quot;&gt;&lt;/a&gt;zookeeper&lt;/h1&gt;&lt;h4 id=&quot;是什么&quot;&gt;&lt;a href=&quot;#是什么&quot; class=&quot;headerlink
      
    
    </summary>
    
      <category term="tech" scheme="http://www.bluedreams.top/categories/tech/"/>
    
    
      <category term="zookeeper" scheme="http://www.bluedreams.top/tags/zookeeper/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://www.bluedreams.top/2018/01/28/%E5%89%8D%E7%AB%AF/Hexo/hello-world/"/>
    <id>http://www.bluedreams.top/2018/01/28/前端/Hexo/hello-world/</id>
    <published>2018-01-28T06:40:22.312Z</published>
    <updated>2018-02-06T06:37:18.878Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
      
    
    </summary>
    
      <category term="前端" scheme="http://www.bluedreams.top/categories/%E5%89%8D%E7%AB%AF/"/>
    
      <category term="Hexo" scheme="http://www.bluedreams.top/categories/%E5%89%8D%E7%AB%AF/Hexo/"/>
    
    
      <category term="前端" scheme="http://www.bluedreams.top/tags/%E5%89%8D%E7%AB%AF/"/>
    
      <category term="Hexo" scheme="http://www.bluedreams.top/tags/Hexo/"/>
    
  </entry>
  
</feed>
